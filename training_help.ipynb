{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 把底模safetensors转化为文件夹权重模式\n",
    "\n",
    "在这里下载底模https://civitai.com/models/27259?modelVersionId=221220然后移动到文件夹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget --content-disposition \"https://civitai.com/api/download/models/221220?type=Model&format=SafeTensor&size=pruned&fp=fp16\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionPipeline\n",
    "\n",
    "checkpoint_path = r\"tmndMix_tmndMixSPRAINBOW.safetensors\"\n",
    "save_path = r\"basemodel\"\n",
    "# 加载 .safetensors 文件\n",
    "pipeline = StableDiffusionPipeline.from_single_file(checkpoint_path)\n",
    "\n",
    "# 将模型保存为 diffusers 格式\n",
    "pipeline.save_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 把数据集解压"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get install unrar\n",
    "!unrar x pic.rar -o+ /"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/sodabreak/PoseCtrl.git\n",
    "!cd poseCtrl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "单卡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python train_colab_V2.py --pretrained_model_name_or_path \"/basemodel\" --base_point_path \"/PoseCtrl/dataSet/standardVertex.txt\" --data_root_path \"/pic\" --train_batch_size 4 --save_steps 2000 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "多卡\n",
    "\n",
    "- 先运行配置\n",
    "- 回答问题：使用混合精度fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerate config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerate launch --num_processes 2 --multi_gpu --mixed_precision \"fp16\" train_colab_V2.py --pretrained_model_name_or_path \"/basemodel\" --base_point_path \"/PoseCtrl/dataSet/standardVertex.txt\" --data_root_path \"/pic\" --train_batch_size 4 --save_steps 2000 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 权重处理\n",
    "\n",
    "所有训练得到的权重都在/sd-pose_ctrl文件夹内\n",
    "\n",
    "把模型大权重转化为训练部分小权重   只要转换model.pth就行，其他权重没用  运行得到posectrl.bin\n",
    "\n",
    "权重处理和inference都能把训练权重下载下来，直接在本机直接运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" V2 \"\"\"\n",
    "import torch\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import os\n",
    "def change_checkpoint(checkpoint_path, new_checkpoint_path):\n",
    "    sd = torch.load(checkpoint_path, map_location=\"cpu\")\n",
    "    image_proj_model_point_sd = {}\n",
    "    atten_sd = {}\n",
    "    proj_sd={}\n",
    "    for k in sd:\n",
    "        if k.startswith(\"unet\"):\n",
    "            pass\n",
    "        elif k.startswith(\"image_proj_model_point\"):\n",
    "            image_proj_model_point_sd[k.replace(\"image_proj_model_point.\", \"\")] = sd[k]\n",
    "        elif k.startswith(\"atten_modules\"):\n",
    "            atten_sd[k.replace(\"atten_modules.\", \"\")] = sd[k]\n",
    "        elif k.startswith(\"image_proj_model\"):\n",
    "            proj_sd[k.replace(\"image_proj_model.\", \"\")] = sd[k]\n",
    "    os.makedirs(new_checkpoint_path, exist_ok=True)\n",
    "    new_checkpoint_path = Path(new_checkpoint_path, \"posectrl.bin\")\n",
    "    torch.save({\"image_proj_model_point\": image_proj_model_point_sd, \"atten_modules\": atten_sd, \"image_proj_model\": proj_sd}, new_checkpoint_path)\n",
    "    print(f\"Saved new checkpoint to {new_checkpoint_path}\")\n",
    "\n",
    "# 修改模型路径\n",
    "ckpt = r\"\"\n",
    "\n",
    "change_checkpoint(ckpt, r\"/result\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" V2 \"\"\"\n",
    "import torch\n",
    "import torch\n",
    "from diffusers import StableDiffusionPipeline, DDIMScheduler, AutoencoderKL\n",
    "from PIL import Image\n",
    "import sys\n",
    "import os\n",
    "notebook_path = os.getcwd()\n",
    "sys.path.append(notebook_path)\n",
    "sys.path.append(os.path.join(notebook_path, \"poseCtrl\"))\n",
    "from poseCtrl.models.pose_adaptor import VPmatrixPoints, ImageProjModel\n",
    "from poseCtrl.models.attention_processor import AttnProcessor, PoseAttnProcessor\n",
    "from poseCtrl.data.dataset import CustomDataset, load_base_points\n",
    "from poseCtrl.models.posectrl import PoseCtrl,PoseCtrlV1,PoseCtrlV2\n",
    "import numpy as np\n",
    "\n",
    "# 加载模型点\n",
    "base_point_path=r'\\dataSet\\standardVertex.txt'\n",
    "raw_base_points=load_base_points(base_point_path)  \n",
    "# 底模路径\n",
    "base_model_path = r\"F:\\Projects\\diffusers\\ProgramData\\basemodel\"\n",
    "vae_model_path = \"stabilityai/sd-vae-ft-mse\"\n",
    "image_encoder_path = \"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\"\n",
    "# 上一步生成的模型权重路径\n",
    "ip_ckpt = r\"F:\\Projects\\diffusers\\Project\\sd-pose_ctrl\\V2\\posectrl.bin\"\n",
    "# 测试集图片路径\n",
    "path = r\"F:\\\\Projects\\\\diffusers\\\\ProgramData\\\\sample_new\"\n",
    "device = \"cuda\"\n",
    "\n",
    "def image_grid(imgs, rows, cols):\n",
    "    assert len(imgs) == rows*cols\n",
    "\n",
    "    w, h = imgs[0].size\n",
    "    grid = Image.new('RGB', size=(cols*w, rows*h))\n",
    "    grid_w, grid_h = grid.size\n",
    "    \n",
    "    for i, img in enumerate(imgs):\n",
    "        grid.paste(img, box=(i%cols*w, i//cols*h))\n",
    "    return grid\n",
    "\n",
    "noise_scheduler = DDIMScheduler(\n",
    "    num_train_timesteps=1000,\n",
    "    beta_start=0.00085,\n",
    "    beta_end=0.012,\n",
    "    beta_schedule=\"scaled_linear\",\n",
    "    clip_sample=False,\n",
    "    set_alpha_to_one=False,\n",
    "    steps_offset=1,\n",
    ")\n",
    "vae = AutoencoderKL.from_pretrained(vae_model_path).to(dtype=torch.float16)\n",
    "\n",
    "# load SD pipeline\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    base_model_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    scheduler=noise_scheduler,\n",
    "    vae=vae,\n",
    "    feature_extractor=None,\n",
    "    safety_checker=None\n",
    ")\n",
    "\n",
    "dataset = CustomDataset(path)\n",
    "\n",
    "# 选择一张图片\n",
    "data = dataset[61]\n",
    "from torchvision import transforms\n",
    "\n",
    "transform = transforms.Resize((256, 256))\n",
    "\n",
    "image = data['image']\n",
    "image_pil = transforms.ToPILImage()(image)\n",
    "image_pil = transform(image_pil)  \n",
    "\n",
    "g_image = data['feature']\n",
    "g_image_pil = transforms.ToPILImage()(g_image)\n",
    "g_image_pil = transform(g_image_pil) \n",
    "\n",
    "vmatrix = data['view_matrix'].to(torch.float16).unsqueeze(0).to(device)\n",
    "pmatrix = data['projection_matrix'].to(torch.float16).unsqueeze(0).to(device)\n",
    "\n",
    "pose_model = PoseCtrlV2(pipe, image_encoder_path, ip_ckpt, raw_base_points, device)\n",
    "images = pose_model.generate(pil_image=g_image, num_samples=4, num_inference_steps=50, seed=42, V_matrix=vmatrix, P_matrix=pmatrix )\n",
    "grid = image_grid(images, 1, 4)\n",
    "grid"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.16 ('llama')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "21aeed30095ae2321904f211b9dcb4b5dfe3fb0d93e1461a079213cbb990a93c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
