{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e2df23a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "xFormers not available\n",
      "xFormers not available\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import csv\n",
    "import sys\n",
    "\n",
    "def read_local_pts(txt_path):\n",
    "    \"\"\"\n",
    "    读取每个输出子文件夹下的 smplx_vs_openpose_joints.txt，返回 numpy 数组 (N, 3)\n",
    "    并交换第1<->2，3<->4个点\n",
    "    \"\"\"\n",
    "    local_pts = []\n",
    "    with open(txt_path, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader)  # 跳过表头\n",
    "        for row in reader:\n",
    "            x, y, z = float(row[3]), float(row[4]), float(row[5])\n",
    "            local_pts.append([x, y, z])\n",
    "\n",
    "    local_pts = np.array(local_pts, dtype=np.float32)\n",
    "\n",
    "    # ✅ 交换第1-2和第3-4个点\n",
    "    # if local_pts.shape[0] >= 4:\n",
    "    #     local_pts[[0, 1]] = local_pts[[1, 0]]\n",
    "    #     local_pts[[2, 3]] = local_pts[[3, 2]]\n",
    "\n",
    "    return local_pts\n",
    "\n",
    "def build_image_model_dict(input_dir, output_dir):\n",
    "    \"\"\"\n",
    "    遍历输入图像，构建包含 image 路径、对应 model.obj 路径和 local_pts 的字典列表\n",
    "    \"\"\"\n",
    "    image_model_list = []\n",
    "\n",
    "    for filename in os.listdir(input_dir):\n",
    "        if filename.lower().endswith((\".jpg\", \".jpeg\", \".png\", \".bmp\")):\n",
    "            image_path = os.path.join(input_dir, filename)\n",
    "\n",
    "            # ✅ 不去掉扩展名\n",
    "            sub_output_dir = os.path.join(output_dir, filename)  # 如 output/00027.png\n",
    "\n",
    "            model_path = os.path.join(sub_output_dir, \"model_transformed.obj\")\n",
    "            joint_txt_path = os.path.join(sub_output_dir, \"smplx_vs_openpose_joints.txt\")\n",
    "\n",
    "            if not (os.path.isfile(model_path) and os.path.isfile(joint_txt_path)):\n",
    "                print(f\"⚠️ 缺失 model 或 joint txt 文件：{sub_output_dir}\")\n",
    "                continue\n",
    "\n",
    "            local_pts = read_local_pts(joint_txt_path)\n",
    "\n",
    "            image_model_list.append({\n",
    "                \"image\": image_path,\n",
    "                \"model\": model_path,\n",
    "                \"local_pts\": local_pts\n",
    "            })\n",
    "\n",
    "    return image_model_list\n",
    "\n",
    "def read_selected_joints2d(txt_path):\n",
    "    \"\"\"\n",
    "    从 selected_joints2d.txt 中读取第 3, 6, 10, 12 行的 (x, y)，返回 numpy 数组 (4, 2)\n",
    "    \"\"\"\n",
    "    selected_indices = [2, 5, 9, 12]\n",
    "    selected_pts = []\n",
    "\n",
    "    with open(txt_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        for idx in selected_indices:\n",
    "            if idx < len(lines):\n",
    "                line = lines[idx].strip().replace(',', ' ')\n",
    "                parts = line.split()\n",
    "                if len(parts) >= 2:\n",
    "                    x, y = float(parts[0]), float(parts[1])\n",
    "                    selected_pts.append([x, y])\n",
    "                else:\n",
    "                    print(f\"⚠️ 第 {idx+1} 行格式错误：{lines[idx]}\")\n",
    "                    selected_pts.append([0.0, 0.0])\n",
    "            else:\n",
    "                print(f\"⚠️ 文件行数不足，缺少第 {idx+1} 行\")\n",
    "                selected_pts.append([0.0, 0.0])\n",
    "\n",
    "    return np.array(selected_pts, dtype=np.float32)\n",
    "\n",
    "def build_image_model_dict_screen(input_dir, output_dir):\n",
    "\n",
    "    \"\"\"\n",
    "    遍历输入图像，构建包含 image 路径、对应 model.obj 路径、\n",
    "    local_pts 和 selected_pts 的字典列表\n",
    "    \"\"\"\n",
    "    image_model_list = []\n",
    "\n",
    "    for filename in os.listdir(input_dir):\n",
    "        if filename.lower().endswith((\".jpg\", \".jpeg\", \".png\", \".bmp\")):\n",
    "            image_path = os.path.join(input_dir, filename)\n",
    "\n",
    "            # ✅ 不去掉扩展名\n",
    "            sub_output_dir = os.path.join(output_dir, filename)  # 如 output/00027.png\n",
    "\n",
    "            model_path = os.path.join(sub_output_dir, \"model_transformed.obj\")\n",
    "            joint_txt_path = os.path.join(sub_output_dir, \"smplx_vs_openpose_joints.txt\")\n",
    "            selected_joints_path = os.path.join(sub_output_dir, \"selected_joints2d.txt\")\n",
    "\n",
    "            # ✅ 检查所有必要文件是否存在\n",
    "            if not (os.path.isfile(model_path) and os.path.isfile(joint_txt_path) and os.path.isfile(selected_joints_path)):\n",
    "                print(f\"⚠️ 缺失必要文件：{sub_output_dir}\")\n",
    "                continue\n",
    "\n",
    "            local_pts = read_local_pts(joint_txt_path)\n",
    "            selected_pts = read_selected_joints2d(selected_joints_path)\n",
    "\n",
    "            image_model_list.append({\n",
    "                \"image\": image_path,\n",
    "                \"model\": model_path,\n",
    "                \"local_pts\": local_pts,\n",
    "                \"selected_pts\": selected_pts\n",
    "            })\n",
    "\n",
    "    return image_model_list\n",
    "\n",
    "def flip_selected_pts_y(selected_pts, image_height):\n",
    "    flipped_pts = selected_pts.copy()\n",
    "    flipped_pts[:, 1] = image_height - flipped_pts[:, 1]\n",
    "    return flipped_pts\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, VitPoseForPoseEstimation\n",
    "from imgutils.detect import detect_person\n",
    "\n",
    "# 提取关键点坐标 (脖子、右肩、左肩、骨盆)\n",
    "def KeyPoint(image_pose_result, image_width, image_height):\n",
    "    if image_pose_result is None or len(image_pose_result) == 0:\n",
    "        return None\n",
    "    keypoints = image_pose_result[0]['keypoints']\n",
    "    return [\n",
    "        # [(keypoints[0][0].item() / image_width), 1-(keypoints[0][1].item() / image_height)],  \n",
    "        # [(keypoints[3][0].item() / image_width), 1-(keypoints[3][1].item() / image_height)],  \n",
    "        # [(keypoints[4][0].item() / image_width), 1-(keypoints[4][1].item() / image_height)],  \n",
    "        [(keypoints[5][0].item() / image_width), 1-(keypoints[5][1].item() / image_height)],  \n",
    "        [(keypoints[6][0].item() / image_width), 1-(keypoints[6][1].item() / image_height)],  \n",
    " \n",
    "        [(keypoints[11][0].item() / image_width), 1-(keypoints[11][1].item() / image_height)], \n",
    "        [(keypoints[12][0].item() / image_width), 1-(keypoints[12][1].item() / image_height)],\n",
    "\n",
    "        # [(keypoints[7][0].item() / image_width), 1-(keypoints[7][1].item() / image_height)],  \n",
    "        # [(keypoints[8][0].item() / image_width), 1-(keypoints[8][1].item() / image_height)] \n",
    "    ]\n",
    "\n",
    "# 检测人物并估计姿态\n",
    "def detect_and_estimate_pose(image_path):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    image = Image.open(image_path)\n",
    "\n",
    "    result = detect_person(image_path)\n",
    "    person_boxes = result[0][0]\n",
    "    if not person_boxes:\n",
    "        print(\"No person detected in the image.\")\n",
    "        return None\n",
    "\n",
    "    person_boxes = np.array(person_boxes, dtype=np.float32)\n",
    "    if person_boxes.ndim == 1:\n",
    "        person_boxes = person_boxes[np.newaxis, :]\n",
    "    person_boxes[:, 2] -= person_boxes[:, 0]\n",
    "    person_boxes[:, 3] -= person_boxes[:, 1]\n",
    "\n",
    "    processor = AutoProcessor.from_pretrained(\"usyd-community/vitpose-base-simple\")\n",
    "    model = VitPoseForPoseEstimation.from_pretrained(\n",
    "        \"usyd-community/vitpose-base-simple\", device_map=device\n",
    "    )\n",
    "    inputs = processor(image, boxes=[person_boxes], return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    pose_results = processor.post_process_pose_estimation(outputs, boxes=[person_boxes])\n",
    "    return pose_results[0] if pose_results else None\n",
    "\n",
    "def make_unity_view_matrix(pos, euler_deg, include_z_flip: bool = True) -> np.ndarray:\n",
    "    pos = np.asarray(pos, dtype=np.float32)\n",
    "    euler_deg = np.asarray(euler_deg, dtype=np.float32)\n",
    "    pitch, yaw, roll = np.deg2rad(euler_deg)\n",
    "    Rz = np.array([[ np.cos(roll), -np.sin(roll), 0], [ np.sin(roll),  np.cos(roll), 0], [ 0, 0, 1]], dtype=np.float32)\n",
    "    Rx = np.array([[1, 0, 0], [0, np.cos(pitch), -np.sin(pitch)], [0, np.sin(pitch), np.cos(pitch)]], dtype=np.float32)\n",
    "    Ry = np.array([[ np.cos(yaw), 0, np.sin(yaw)], [0, 1, 0], [-np.sin(yaw), 0, np.cos(yaw)]], dtype=np.float32)\n",
    "    Rcw = Ry @ Rx @ Rz\n",
    "    view_rot = Rcw.T\n",
    "    view_trans = -view_rot @ pos\n",
    "    view = np.eye(4, dtype=np.float32)\n",
    "    view[:3, :3] = view_rot\n",
    "    view[:3,  3] = view_trans\n",
    "    if include_z_flip:\n",
    "        flipZ = np.diag([1, 1, -1, 1]).astype(np.float32)\n",
    "        view = flipZ @ view\n",
    "    return view\n",
    "\n",
    "def perspective_projection(fov_deg, aspect, near, far) -> np.ndarray:\n",
    "    f = 1.0 / np.tan(np.deg2rad(fov_deg) * 0.5)\n",
    "    M = np.zeros((4,4), dtype=np.float32)\n",
    "    M[0,0] = f / aspect\n",
    "    M[1,1] = f\n",
    "    M[2,2] = -(far / (far - near))\n",
    "    M[2,3] = near * far / (far - near)\n",
    "    M[3,2] = -1.0\n",
    "    return M\n",
    "\n",
    "def project_point(point_3d, camera_params, image_width=1024, image_height=1024, fov_deg=60.0, fixed_roll=0.0):\n",
    "    if len(camera_params) == 5:\n",
    "        px, py, pz, pitch, yaw = camera_params\n",
    "        roll = fixed_roll\n",
    "    else:\n",
    "        px, py, pz, pitch, yaw, roll = camera_params\n",
    "\n",
    "    view = make_unity_view_matrix([px, py, pz], [pitch, yaw, roll], include_z_flip=True)\n",
    "    aspect = image_width / image_height\n",
    "    proj = perspective_projection(fov_deg, aspect, near=0.1, far=1000.0)\n",
    "    # print(view)\n",
    "    # print(proj)\n",
    "    mvp = proj @ view\n",
    "    pt_h = np.array([*point_3d, 1.0], dtype=np.float32)\n",
    "    clip = mvp @ pt_h\n",
    "    ndc = clip[:3] / (clip[3] if clip[3] != 0 else 1)\n",
    "    sx = (ndc[0] + 1) * 0.5 #适配右手系先这样，因为\n",
    "    sy = (ndc[1] + 1) * 0.5\n",
    "    return np.array([sx, sy])\n",
    "\n",
    "# 加入方向比例残差\n",
    "def pairwise_vector_ratios(points):\n",
    "    p = np.array(points)\n",
    "    v01 = p[1] - p[0]\n",
    "    v23 = p[3] - p[2]\n",
    "    v02 = p[2] - p[0]\n",
    "    v13 = p[3] - p[1]\n",
    "    ratios = np.array([\n",
    "        v01[0], v01[1],\n",
    "        v23[0], v23[1],\n",
    "        v02[0], v02[1],\n",
    "        v13[0], v13[1],\n",
    "    ], dtype=np.float32)\n",
    "    return ratios\n",
    "\n",
    "def compute_residuals(camera_params, local_points, screen_points, image_width=1024,image_height=1024,fixed_roll=0.0, ratio_weight=0):\n",
    "    residuals = []\n",
    "    projected_points = []\n",
    "    for i, (local, screen) in enumerate(zip(local_points, screen_points)):\n",
    "        proj = project_point(local, camera_params,image_width=image_width,image_height=image_height, fixed_roll=fixed_roll)\n",
    "        projected_points.append(proj)\n",
    "        # 对前三个点应用权重，其余保持原权重（1.0）\n",
    "        if i < 3:\n",
    "            weight = ratio_weight\n",
    "        if i > 6:\n",
    "            weight=0.5\n",
    "        else:\n",
    "            weight = 1.0\n",
    "        # weight = ratio_weight if i < 3 else 1.0  #head weight\n",
    "        residuals.extend((proj - screen) * weight)\n",
    "\n",
    "    return np.array(residuals, dtype=np.float32)\n",
    "\n",
    "def compute_jacobian(camera_params, local_points, screen_points,image_width=1024,image_height=1024, fixed_roll=0.0, ratio_weight=0):\n",
    "    n_params = len(camera_params)\n",
    "    eps = 1e-6 * (1 + np.abs(camera_params))\n",
    "    base = compute_residuals(camera_params, local_points, screen_points,image_width=image_width,image_height=image_height, fixed_roll=fixed_roll, ratio_weight=ratio_weight)\n",
    "    J = np.zeros((base.size, n_params), dtype=np.float32)\n",
    "    for i in range(n_params):\n",
    "        p2 = camera_params.copy()\n",
    "        p2[i] += eps[i]\n",
    "        r2 = compute_residuals(p2, local_points, screen_points,image_width=image_width,image_height=image_height, fixed_roll=fixed_roll, ratio_weight=ratio_weight)\n",
    "        J[:, i] = (r2 - base) / eps[i]\n",
    "    return J\n",
    "\n",
    "def levenberg_marquardt_optimization(camera_params, local_points, screen_points,image_width=1024,image_height=1024, max_iter=200, tol=1e-6, fixed_roll=0.0, ratio_weight=0):\n",
    "    lam = 1e-3\n",
    "    cost = np.sum(compute_residuals(camera_params, local_points, screen_points,image_width=image_width,image_height=image_height, fixed_roll=fixed_roll, ratio_weight=ratio_weight)**2)\n",
    "    for _ in range(max_iter):\n",
    "        r = compute_residuals(camera_params, local_points, screen_points,image_width=image_width,image_height=image_height, fixed_roll=fixed_roll, ratio_weight=ratio_weight)\n",
    "        J = compute_jacobian(camera_params, local_points, screen_points,image_width=image_width,image_height=image_height, fixed_roll=fixed_roll, ratio_weight=ratio_weight)\n",
    "        A = J.T @ J\n",
    "        g = J.T @ r\n",
    "        diag = np.diag(A)\n",
    "        L = lam * np.diag(diag + 1e-6)\n",
    "        try:\n",
    "            dp = np.linalg.solve(A + L, -g)\n",
    "        except np.linalg.LinAlgError:\n",
    "            break\n",
    "        new_p = camera_params + dp\n",
    "        new_cost = np.sum(compute_residuals(new_p, local_points, screen_points,image_width=image_width,image_height=image_height, fixed_roll=fixed_roll, ratio_weight=ratio_weight)**2)\n",
    "        if new_cost < cost:\n",
    "            camera_params = new_p\n",
    "            cost = new_cost\n",
    "            lam *= 0.1\n",
    "            if np.linalg.norm(dp) < tol:\n",
    "                break\n",
    "        else:\n",
    "            lam *= 10\n",
    "    return camera_params\n",
    "\n",
    "def left_to_right(local_pts):\n",
    "    return local_pts * np.array([-1, 1, 1], dtype=np.float32)\n",
    "\n",
    "def estimate_camera_pose_from_image(image_path, local_pts, fixed_roll=0.0, ratio_weight=0.0):\n",
    "    \"\"\"\n",
    "    根据图像路径和3D关键点，估计并优化相机位姿（固定 roll），返回优化后的参数。\n",
    "    \n",
    "    参数:\n",
    "        image_path (str): 图像文件路径\n",
    "        local_pts (np.ndarray): 3D 本地坐标点数组，形状为 (N, 3)\n",
    "        fixed_roll (float): 固定的相机 roll 角，默认 0.0\n",
    "        ratio_weight (float): 比例项残差权重，默认 0.0\n",
    "    \n",
    "    返回:\n",
    "        np.ndarray: 优化后的相机参数（长度为6的数组：[x, y, z, pitch, yaw]）\n",
    "    \"\"\"\n",
    "    # 获取图像尺寸\n",
    "    with Image.open(image_path) as img:\n",
    "        image_width, image_height = img.size\n",
    "\n",
    "    # 估计2D关键点\n",
    "    pose_res = detect_and_estimate_pose(image_path)\n",
    "    screen_pts = KeyPoint(pose_res, image_width, image_height)\n",
    "\n",
    "    # 左右坐标转换\n",
    "    local_pts = left_to_right(local_pts)\n",
    "\n",
    "    # 初始化相机参数\n",
    "    init_pos = np.array([0.0, 0.0, 2.0], dtype=np.float32)\n",
    "    init_rot = np.array([0.0, 180.0], dtype=np.float32)  # pitch, yaw\n",
    "    camera_params = np.concatenate([init_pos, init_rot])  # 不包括 roll\n",
    "\n",
    "    # 优化\n",
    "    opt_params = levenberg_marquardt_optimization(\n",
    "        camera_params, local_pts, np.array(screen_pts),\n",
    "        image_width=image_width,\n",
    "        image_height=image_height,\n",
    "        fixed_roll=fixed_roll,\n",
    "        ratio_weight=ratio_weight\n",
    "    )\n",
    "\n",
    "    return opt_params\n",
    "\n",
    "def maybe_reverse_result(render_result, result):\n",
    "    \"\"\"\n",
    "    判断 render_result 的 keypoint_0 和 keypoint_1 顺序是否与 result 相反。\n",
    "    \n",
    "    返回:\n",
    "    - True: 需要反转\n",
    "    - False: 保持不变\n",
    "        \"\"\"\n",
    "    r0, r1 = render_result['keypoint_0'], render_result['keypoint_1']\n",
    "    o0, o1 = result['keypoint_0'], result['keypoint_1']\n",
    "\n",
    "\n",
    "    # 判断是否方向相反\n",
    "    opposite = (r0 > r1 and o0 < o1) or (r0 < r1 and o0 > o1)\n",
    "    return opposite\n",
    "def extract_given_points_depths(image_path, keypoints_norm):\n",
    "    \"\"\"\n",
    "    输入图像路径和归一化二维点，输出这些点的平均深度值。\n",
    "    \n",
    "    参数:\n",
    "    - image_path: 图像路径\n",
    "    - keypoints_norm: 归一化二维点列表，如 [[x1, y1], [x2, y2], ...]，范围在 0~1\n",
    "    \n",
    "    返回:\n",
    "    - 字典，键为 'keypoint_0' 等，值为平均深度\n",
    "    \"\"\"\n",
    "    # 加载模型\n",
    "    model = load_depth_model()\n",
    "\n",
    "    # 获取图像尺寸\n",
    "    with Image.open(image_path) as img:\n",
    "        image_width, image_height = img.size\n",
    "    raw_img = cv2.imread(image_path)\n",
    "\n",
    "    # 深度推理\n",
    "    depth = model.infer_image(raw_img)\n",
    "\n",
    "    # 计算关键点深度\n",
    "    mean_depths = {}\n",
    "    for idx in [0, 1, 2, 3]:\n",
    "        mean_depths[f'keypoint_{idx}'] = get_mean_depth(depth, keypoints_norm[idx], image_width, image_height, kernel=1,if_screen_pos=False)\n",
    "\n",
    "    return mean_depths\n",
    "\n",
    "SYS_PATH = r\"C:\\Users\\31878\\Desktop\\pose_model\\PoseCtrl-main\\Depth-Anything-V2\"\n",
    "sys.path.insert(0, SYS_PATH)\n",
    "\n",
    "from depth_anything_v2.dpt import DepthAnythingV2\n",
    "\n",
    "def load_depth_model():\n",
    "    \"\"\"加载深度预测模型\"\"\"\n",
    "    model = DepthAnythingV2(encoder='vitl', features=256, out_channels=[256, 512, 1024, 1024])\n",
    "    model.load_state_dict(torch.load(\n",
    "        r\"C:\\Users\\31878\\Desktop\\pose_model\\PoseCtrl-main\\Depth-Anything-V2\\checkpoints\\depth_anything_v2_vitl.pth\",\n",
    "        map_location='cpu'\n",
    "    ))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def get_mean_depth(depth_image, keypoint, image_width, image_height, kernel=1, if_screen_pos=False):\n",
    "    \"\"\"\n",
    "    计算 keypoint 周围区域的平均深度。\n",
    "\n",
    "    参数:\n",
    "    - depth_image: 2D numpy array，深度图\n",
    "    - keypoint: 坐标点\n",
    "    - image_width, image_height: 图像宽高\n",
    "    - kernel: 邻域半径\n",
    "    - if_screen_pos: bool，True 表示 keypoint 是像素坐标（不用再乘图像大小），\n",
    "                     False 表示 keypoint 是归一化坐标（0~1），需要乘图像大小转换成像素坐标\n",
    "    \"\"\"\n",
    "    if if_screen_pos:\n",
    "        cx = int(round(keypoint[0] * image_width))\n",
    "        cy = int(round((1 - keypoint[1]) * image_height))\n",
    "\n",
    "    else:\n",
    "        cx = keypoint[0]\n",
    "        cy = keypoint[1]\n",
    "\n",
    "\n",
    "    h, w = depth_image.shape\n",
    "\n",
    "    vals = []\n",
    "    for dy in range(-kernel, kernel + 1):\n",
    "        for dx in range(-kernel, kernel + 1):\n",
    "            x = cx + dx\n",
    "            y = cy + dy\n",
    "            if 0 <= x < w and 0 <= y < h:\n",
    "                vals.append(float(depth_image[y, x]))\n",
    "    if not vals:\n",
    "        return None\n",
    "    return sum(vals) / len(vals)\n",
    "\n",
    "\n",
    "def extract_keypoints_depths(image_path):\n",
    "    \"\"\"\n",
    "    输入图像路径，输出关键点的平均深度值（索引 5~8）。\n",
    "    \"\"\"\n",
    "    # 加载模型\n",
    "    model = load_depth_model()\n",
    "\n",
    "    # 读取图像\n",
    "    with Image.open(image_path) as img:\n",
    "        image_width, image_height = img.size\n",
    "    raw_img = cv2.imread(image_path)\n",
    "\n",
    "    # 替换为你自己的关键点检测函数\n",
    "    pose_res = detect_and_estimate_pose(image_path)\n",
    "    screen_pts = KeyPoint(pose_res, image_width, image_height)\n",
    "    if screen_pts is None:\n",
    "        print(\"未检测到人体关键点。\")\n",
    "        return None\n",
    "\n",
    "    # 深度推理\n",
    "    depth = model.infer_image(raw_img)\n",
    "\n",
    "    # 计算关键点深度\n",
    "    mean_depths = {}\n",
    "    for idx in [0, 1, 2, 3]:\n",
    "        mean_depths[f'keypoint_{idx}'] = get_mean_depth(depth, screen_pts[idx], image_width, image_height, kernel=1,if_screen_pos=True)\n",
    "\n",
    "    return mean_depths\n",
    "\n",
    "\n",
    "import math\n",
    "\n",
    "def mirror_point_about_yz_plane(point):\n",
    "    x, y, z = point\n",
    "    return (-x, y, z)\n",
    "\n",
    "def mirror_vector_about_yz_plane(vec):\n",
    "    return mirror_point_about_yz_plane(vec)\n",
    "\n",
    "def euler_to_forward(pitch, yaw):\n",
    "    pitch_rad = math.radians(pitch)\n",
    "    yaw_rad = math.radians(yaw)\n",
    "    x = math.sin(yaw_rad) * math.cos(pitch_rad)\n",
    "    y = math.sin(pitch_rad)\n",
    "    z = math.cos(yaw_rad) * math.cos(pitch_rad)\n",
    "    return (x, y, z)\n",
    "\n",
    "def forward_to_euler(forward):\n",
    "    x, y, z = forward\n",
    "    yaw = math.degrees(math.atan2(x, z))\n",
    "    hyp = math.sqrt(x*x + z*z)\n",
    "    pitch = math.degrees(math.atan2(y, hyp))\n",
    "    return (pitch, yaw)\n",
    "\n",
    "def mirror_camera(camera_pos, camera_euler):\n",
    "    # 镜像位置\n",
    "    new_pos = mirror_point_about_yz_plane(camera_pos)\n",
    "\n",
    "    pitch, yaw = camera_euler\n",
    "    roll=0\n",
    "    forward_vec = euler_to_forward(pitch, yaw)\n",
    "\n",
    "    # 镜像观察向量\n",
    "    mirrored_forward = mirror_vector_about_yz_plane(forward_vec)\n",
    "\n",
    "    # 转换回欧拉角\n",
    "    new_pitch, new_yaw = forward_to_euler(mirrored_forward)\n",
    "\n",
    "    # roll取负\n",
    "    new_roll = -roll\n",
    "\n",
    "    return new_pos + (new_pitch, new_yaw % 360, new_roll % 360)\n",
    "\n",
    "# from unity_controller import UnityController\n",
    "# from functools import lru_cache\n",
    "# from concurrent.futures import ThreadPoolExecutor\n",
    "# controller = UnityController()\n",
    "\n",
    "# image = controller.render_obj(\n",
    "#     obj_path=r\"C:\\Users\\31878\\Desktop\\output\\3.png_002\\3.obj\",\n",
    "#     camera_position=(0, 0, 2),\n",
    "#     camera_rotation=(0, 180),\n",
    "#     resolution=(1024, 1024)\n",
    "# )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72f66a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "C:\\Users\\31878\\AppData\\Local\\Temp\\ipykernel_21516\\3808249763.py:280: RuntimeWarning: overflow encountered in multiply\n",
      "  L = lam * np.diag(diag + 1e-6)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.183033  0.019016  0.057625]\n",
      " [-0.151864  0.056067  0.071633]\n",
      " [ 0.055617 -0.490661 -0.009767]\n",
      " [-0.056351 -0.478296 -0.015449]]\n"
     ]
    },
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[WinError 10061] 由于目标计算机积极拒绝，无法连接。",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 25\u001b[0m\n\u001b[0;32m     21\u001b[0m     image_width, image_height \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39msize\n\u001b[0;32m     23\u001b[0m opt_params \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mfloat\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m opt_params]\n\u001b[1;32m---> 25\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mcontroller\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender_obj\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m\u001b[49m\u001b[43mobj_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m\u001b[49m\u001b[43mcamera_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mopt_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m\u001b[49m\u001b[43mcamera_rotation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mopt_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m\u001b[49m\u001b[43mresolution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mimage_width\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_height\u001b[49m\u001b[43m)\u001b[49m\u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(item[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m])              \u001b[38;5;66;03m# 获取文件名 例如 00027.png\u001b[39;00m\n\u001b[0;32m     32\u001b[0m save_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m31878\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124munity_image\u001b[39m\u001b[38;5;124m\"\u001b[39m, filename)\n",
      "File \u001b[1;32mc:\\Users\\31878\\Desktop\\pose_model\\PoseCtrl-main\\unity_controller.py:56\u001b[0m, in \u001b[0;36mUnityController.render_obj\u001b[1;34m(self, obj_path, camera_position, camera_rotation, resolution)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrender_obj\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     44\u001b[0m                obj_path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m     45\u001b[0m                camera_position: \u001b[38;5;28mtuple\u001b[39m,\n\u001b[0;32m     46\u001b[0m                camera_rotation: \u001b[38;5;28mtuple\u001b[39m,\n\u001b[0;32m     47\u001b[0m                resolution: \u001b[38;5;28mtuple\u001b[39m \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1024\u001b[39m, \u001b[38;5;241m1024\u001b[39m)) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Image\u001b[38;5;241m.\u001b[39mImage:\n\u001b[0;32m     48\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03m    加载并渲染一个 OBJ 模型\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;124;03m    :param obj_path: 模型的绝对路径（.obj）\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;124;03m    :return: PIL.Image.Image\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrender_obj\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mobjPath\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcamera_position\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43my\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcamera_position\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mz\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcamera_position\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcamera_rotation\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mry\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcamera_rotation\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwidth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresolution\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mheight\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresolution\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\31878\\Desktop\\pose_model\\PoseCtrl-main\\unity_controller.py:25\u001b[0m, in \u001b[0;36mUnityController.send_command\u001b[1;34m(self, command_type, data)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"发送命令，接收响应（如图像）\"\"\"\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m socket\u001b[38;5;241m.\u001b[39msocket(socket\u001b[38;5;241m.\u001b[39mAF_INET, socket\u001b[38;5;241m.\u001b[39mSOCK_STREAM) \u001b[38;5;28;01mas\u001b[39;00m s:\n\u001b[1;32m---> 25\u001b[0m     \u001b[43ms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;66;03m# 发送JSON命令\u001b[39;00m\n\u001b[0;32m     28\u001b[0m     command_json \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mdumps({\n\u001b[0;32m     29\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: command_type,\n\u001b[0;32m     30\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m: data\n\u001b[0;32m     31\u001b[0m     })\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mConnectionRefusedError\u001b[0m: [WinError 10061] 由于目标计算机积极拒绝，无法连接。"
     ]
    }
   ],
   "source": [
    "from unity_controller import UnityController\n",
    "from functools import lru_cache\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "controller = UnityController()\n",
    "input_dir = r\"C:\\Users\\31878\\Desktop\\test\\input\"\n",
    "output_dir = r\"C:\\Users\\31878\\Desktop\\test\\output\"\n",
    "\n",
    "result = build_image_model_dict(input_dir, output_dir)\n",
    "\n",
    "# 查看结果\n",
    "for item in result:\n",
    "    local_pts = left_to_right(item[\"local_pts\"])\n",
    "    opt_params = estimate_camera_pose_from_image(item[\"image\"], local_pts)\n",
    "    print(local_pts)\n",
    "    # print(\"图像:\", item[\"image\"])\n",
    "    # print(\"模型:\", item[\"model\"])\n",
    "    # print(\"local_pts:\\n\", item[\"local_pts\"])\n",
    "    # print(\"opt_params:\\n\", opt_params)\n",
    "\n",
    "    with Image.open(item[\"image\"]) as img:\n",
    "        image_width, image_height = img.size\n",
    "        \n",
    "    opt_params = [float(x) for x in opt_params]\n",
    "\n",
    "    image = controller.render_obj(\n",
    "    obj_path=item[\"model\"],\n",
    "    camera_position=(opt_params[0], opt_params[1], opt_params[2]),\n",
    "    camera_rotation=(opt_params[3], opt_params[4]),\n",
    "    resolution=(image_width, image_height)    )\n",
    "\n",
    "    filename = os.path.basename(item[\"image\"])              # 获取文件名 例如 00027.png\n",
    "    save_path = os.path.join(r\"C:\\Users\\31878\\Desktop\\test\\unity_image\", filename)\n",
    "    image.save(save_path)\n",
    "\n",
    "    \n",
    "    # ori_image_point = [None] * len(local_pts)  # 初始化列表，长度和local_pts一样\n",
    "    # # ori_pixel_points = [None] * len(item[\"local_pts\"])\n",
    "    # ori_pixel_points = []\n",
    "    # for i, pt in enumerate(local_pts):\n",
    "    #     ori_image_point = project_point(pt, opt_params, image_width=image_width, image_height=image_height, fixed_roll=0.0)\n",
    "    #     pixel = (\n",
    "    #         int(round(ori_image_point[0] * image_width)),\n",
    "    #         int(round(ori_image_point[1] * image_height))\n",
    "    #     )\n",
    "    #     # print(pixel)\n",
    "    #     ori_pixel_points.append(pixel)\n",
    "\n",
    "\n",
    "    # ##渲染图\n",
    "    # print(save_path)\n",
    "    # print(ori_pixel_points)\n",
    "    \n",
    "    # render_result=extract_given_points_depths(save_path, ori_pixel_points)\n",
    "    # ##原图\n",
    "    # result = extract_keypoints_depths(item[\"image\"])\n",
    "\n",
    "    # print(type(render_result), render_result)\n",
    "    # print(type(result), result)\n",
    "    \n",
    "    # if maybe_reverse_result(render_result, result):\n",
    "    #     print(filename+\" inverse\")\n",
    "    #     new_opt_params = mirror_camera((opt_params[0], opt_params[1], opt_params[2]), (opt_params[3], opt_params[4]))\n",
    "    #     image = controller.render_obj(\n",
    "    #     obj_path=item[\"model\"],\n",
    "    #     camera_position=(new_opt_params[0], new_opt_params[1], new_opt_params[2]),\n",
    "    #     camera_rotation=(new_opt_params[3], new_opt_params[4]),\n",
    "    #     resolution=(image_width, image_height)    )\n",
    "\n",
    "    #     filename = os.path.basename(item[\"image\"])              # 获取文件名 例如 00027.png\n",
    "    #     save_path = os.path.join(r\"C:\\Users\\31878\\Desktop\\test\\unity_image\", filename)\n",
    "    #     image.save(save_path)\n",
    "\n",
    "\n",
    "    print(\"=\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95444a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# 可微版本 perspective projection\n",
    "def perspective_projection_torch(fov_deg, aspect, near, far):\n",
    "    f = 1.0 / torch.tan(torch.deg2rad(torch.tensor(fov_deg * 0.5)))\n",
    "    M = torch.zeros(4, 4, dtype=torch.float32)\n",
    "    M[0, 0] = f / aspect\n",
    "    M[1, 1] = f\n",
    "    M[2, 2] = -(far / (far - near))\n",
    "    M[2, 3] = near * far / (far - near)\n",
    "    M[3, 2] = -1.0\n",
    "    return M\n",
    "\n",
    "def make_unity_view_matrix_torch(pos, euler_deg, include_z_flip=True):\n",
    "    pitch, yaw, roll = euler_deg  # pitch, yaw, roll 必须是 torch.Tensor\n",
    "\n",
    "    cp, sp = torch.cos(torch.deg2rad(pitch)), torch.sin(torch.deg2rad(pitch))\n",
    "    cy, sy = torch.cos(torch.deg2rad(yaw)), torch.sin(torch.deg2rad(yaw))\n",
    "    cr, sr = torch.cos(torch.deg2rad(roll)), torch.sin(torch.deg2rad(roll))\n",
    "\n",
    "    device = pitch.device  # 获取当前设备\n",
    "\n",
    "    # ✅ 所有常数变成 tensor\n",
    "    zero = torch.tensor(0.0, device=device)\n",
    "    one = torch.tensor(1.0, device=device)\n",
    "\n",
    "    Rz = torch.stack([\n",
    "        torch.stack([ cr, -sr, zero]),\n",
    "        torch.stack([ sr,  cr, zero]),\n",
    "        torch.stack([zero, zero, one])\n",
    "    ])\n",
    "\n",
    "    Rx = torch.stack([\n",
    "        torch.stack([one, zero, zero]),\n",
    "        torch.stack([zero,  cp, -sp]),\n",
    "        torch.stack([zero,  sp,  cp])\n",
    "    ])\n",
    "\n",
    "    Ry = torch.stack([\n",
    "        torch.stack([ cy, zero,  sy]),\n",
    "        torch.stack([zero,  one, zero]),\n",
    "        torch.stack([-sy, zero,  cy])\n",
    "    ])\n",
    "\n",
    "    R = Ry @ Rx @ Rz\n",
    "    view_rot = R.T\n",
    "    view_trans = -view_rot @ pos\n",
    "\n",
    "    view = torch.eye(4, device=device)\n",
    "    view[:3, :3] = view_rot\n",
    "    view[:3, 3] = view_trans\n",
    "\n",
    "    if include_z_flip:\n",
    "        flipZ = torch.diag(torch.tensor([1, 1, -1, 1], dtype=torch.float32, device=device))\n",
    "        view = flipZ @ view\n",
    "\n",
    "    return view\n",
    "\n",
    "\n",
    "def project_point_torch(point_3d, camera_params, image_width, image_height, fov_deg=60.0):\n",
    "    px, py, pz, pitch, yaw = camera_params\n",
    "    roll = torch.tensor(0.0)\n",
    "    pos = torch.stack([px, py, pz])\n",
    "    angles = torch.stack([pitch, yaw, roll])\n",
    "\n",
    "    view = make_unity_view_matrix_torch(pos, angles)\n",
    "    aspect = image_width / image_height\n",
    "    proj = perspective_projection_torch(fov_deg, aspect, near=0.1, far=1000.0)\n",
    "    mvp = proj @ view\n",
    "\n",
    "    pt_h = torch.cat([point_3d, torch.tensor([1.0])])\n",
    "    clip = mvp @ pt_h\n",
    "    ndc = clip[:3] / clip[3]\n",
    "    sx = (ndc[0] + 1) * 0.5\n",
    "    sy = (ndc[1] + 1) * 0.5\n",
    "    return torch.stack([sx, sy])\n",
    "\n",
    "# ==== 优化入口 ====\n",
    "\n",
    "def optimize_camera_torch(\n",
    "    local_pts, selected_pts, image_width, image_height,\n",
    "    fov_deg=60.0, lr=1e-2, iterations=300\n",
    "):\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "    local_pts = torch.tensor(local_pts, dtype=torch.float32, device=device)\n",
    "    selected_pts = torch.tensor(selected_pts, dtype=torch.float32, device=device)\n",
    "    selected_pts = selected_pts / torch.tensor([image_width, image_height], dtype=torch.float32)\n",
    "\n",
    "    # 初始相机参数 px, py, pz, pitch, yaw\n",
    "    centroid = local_pts.mean(dim=0)\n",
    "    init_params = torch.tensor([\n",
    "        centroid[0],\n",
    "        centroid[1],\n",
    "        centroid[2] + 2.0,\n",
    "        0,     # 初始 pitch，略有扰动\n",
    "        180.0     # 初始 yaw\n",
    "    ], dtype=torch.float32)\n",
    "    camera_params = nn.Parameter(init_params.clone(), requires_grad=True)\n",
    "\n",
    "    optimizer = optim.Adam([camera_params], lr=lr)\n",
    "\n",
    "    for i in range(iterations):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 拆解 camera 参数（保持计算图）\n",
    "        px, py, pz, pitch, yaw = camera_params\n",
    "        pos = torch.stack([px, py, pz])\n",
    "\n",
    "        # roll 固定值，但放在同图中，避免断图\n",
    "        roll = torch.zeros(1, device=camera_params.device)[0]  # 不用 tensor(0.0)，会断计算图！\n",
    "        angles = torch.stack([pitch, yaw, roll])  # 不打断计算图\n",
    "\n",
    "        # 得到 view 和 projection 矩阵\n",
    "        view = make_unity_view_matrix_torch(pos, angles)\n",
    "        aspect = image_width / image_height\n",
    "        proj = perspective_projection_torch(fov_deg, aspect, near=0.1, far=1000.0)\n",
    "        mvp = proj @ view\n",
    "\n",
    "        # 批量投影\n",
    "        ones = torch.ones(local_pts.shape[0], 1, device=local_pts.device)\n",
    "        pts_homo = torch.cat([local_pts, ones], dim=1)\n",
    "        clip = (mvp @ pts_homo.T).T\n",
    "        ndc = clip[:, :3] / clip[:, 3:4]\n",
    "        screen = (ndc[:, :2] + 1.0) * 0.5\n",
    "\n",
    "        # Loss: 平均像素误差\n",
    "        loss = ((screen - selected_pts) ** 2).mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 10 == 0 or i == iterations - 1:\n",
    "            print(f\"[{i}] Loss: {loss.item():.6f}, Params: {camera_params.data.tolist()}\")\n",
    "\n",
    "    return camera_params.data.detach().cpu().numpy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e6c879a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] Loss: 0.015454, Params: [0.002391248010098934, -0.2134685218334198, 2.0160105228424072, -0.009999928064644337, 179.99000549316406]\n",
      "[10] Loss: 0.007659, Params: [0.09723693132400513, -0.1166161596775055, 1.9143435955047607, -0.10679274797439575, 179.89547729492188]\n",
      "[20] Loss: 0.006189, Params: [0.1547548472881317, -0.041153088212013245, 1.8214061260223389, -0.18320006132125854, 179.83848571777344]\n",
      "[30] Loss: 0.006282, Params: [0.14743460714817047, -0.01416033785790205, 1.7959777116775513, -0.21623750030994415, 179.84378051757812]\n",
      "[40] Loss: 0.006044, Params: [0.12037810683250427, -0.028240254148840904, 1.8091837167739868, -0.21354253590106964, 179.868408203125]\n",
      "[50] Loss: 0.006010, Params: [0.11718661338090897, -0.047277260571718216, 1.789986252784729, -0.20739829540252686, 179.8715057373047]\n",
      "[60] Loss: 0.005966, Params: [0.12553799152374268, -0.05332431569695473, 1.7477352619171143, -0.21373961865901947, 179.8642120361328]\n",
      "[70] Loss: 0.005954, Params: [0.12282471358776093, -0.05214521661400795, 1.7185648679733276, -0.2272079437971115, 179.8670196533203]\n",
      "[80] Loss: 0.005948, Params: [0.11694619059562683, -0.052677612751722336, 1.7053056955337524, -0.2400769740343094, 179.8728485107422]\n",
      "[90] Loss: 0.005946, Params: [0.11773917078971863, -0.055202558636665344, 1.6970417499542236, -0.25212645530700684, 179.8730010986328]\n",
      "[100] Loss: 0.005945, Params: [0.11874086409807205, -0.05668298155069351, 1.6947780847549438, -0.2658202648162842, 179.87307739257812]\n",
      "[110] Loss: 0.005944, Params: [0.11786986142396927, -0.056609395891427994, 1.6966525316238403, -0.2815391421318054, 179.87486267089844]\n",
      "[120] Loss: 0.005943, Params: [0.11816149204969406, -0.05651649460196495, 1.6980630159378052, -0.29800698161125183, 179.8756866455078]\n",
      "[130] Loss: 0.005942, Params: [0.11857824772596359, -0.05706381797790527, 1.699324607849121, -0.31464883685112, 179.87646484375]\n",
      "[140] Loss: 0.005941, Params: [0.11847604811191559, -0.05761859193444252, 1.7002209424972534, -0.33195987343788147, 179.8777618408203]\n",
      "[150] Loss: 0.005940, Params: [0.11855726689100266, -0.05803024023771286, 1.7001187801361084, -0.3500431180000305, 179.87889099121094]\n",
      "[160] Loss: 0.005939, Params: [0.11859985440969467, -0.05860401690006256, 1.6999320983886719, -0.3686214089393616, 179.880126953125]\n",
      "[170] Loss: 0.005938, Params: [0.11858943104743958, -0.059238821268081665, 1.6998534202575684, -0.38775739073753357, 179.88150024414062]\n",
      "[180] Loss: 0.005937, Params: [0.11864306777715683, -0.05981677025556564, 1.6997480392456055, -0.40753060579299927, 179.8828125]\n",
      "[190] Loss: 0.005935, Params: [0.11867323517799377, -0.060415081679821014, 1.6997662782669067, -0.427860289812088, 179.88418579101562]\n",
      "[200] Loss: 0.005934, Params: [0.11871518939733505, -0.06104074791073799, 1.699823260307312, -0.4487260580062866, 179.88555908203125]\n",
      "[210] Loss: 0.005933, Params: [0.11876166611909866, -0.06167163699865341, 1.6998649835586548, -0.470132052898407, 179.88693237304688]\n",
      "[220] Loss: 0.005932, Params: [0.118801549077034, -0.06231961026787758, 1.6999006271362305, -0.49205538630485535, 179.88845825195312]\n",
      "[230] Loss: 0.005930, Params: [0.11884940415620804, -0.06298729032278061, 1.6999211311340332, -0.5144814252853394, 179.88998413085938]\n",
      "[240] Loss: 0.005929, Params: [0.11889233440160751, -0.06366860866546631, 1.6999441385269165, -0.5374035239219666, 179.89151000976562]\n",
      "[250] Loss: 0.005928, Params: [0.11893672496080399, -0.06436318159103394, 1.6999667882919312, -0.5608111619949341, 179.89303588867188]\n",
      "[260] Loss: 0.005926, Params: [0.11898161470890045, -0.06507276743650436, 1.6999918222427368, -0.5846922397613525, 179.8946075439453]\n",
      "[270] Loss: 0.005925, Params: [0.11903052031993866, -0.06579583883285522, 1.7000148296356201, -0.6090384721755981, 179.8962860107422]\n",
      "[280] Loss: 0.005924, Params: [0.11908009648323059, -0.06653250753879547, 1.7000397443771362, -0.6338403820991516, 179.89796447753906]\n",
      "[290] Loss: 0.005922, Params: [0.1191285029053688, -0.06728225201368332, 1.7000670433044434, -0.6590894460678101, 179.89964294433594]\n",
      "[300] Loss: 0.005921, Params: [0.1191779226064682, -0.06804487854242325, 1.7000932693481445, -0.6847772598266602, 179.9013214111328]\n",
      "[310] Loss: 0.005919, Params: [0.1192266196012497, -0.06882062554359436, 1.7001194953918457, -0.7108955979347229, 179.9030303955078]\n",
      "[320] Loss: 0.005918, Params: [0.11927993595600128, -0.0696089044213295, 1.7001420259475708, -0.7374371290206909, 179.9048614501953]\n",
      "[330] Loss: 0.005916, Params: [0.11933358013629913, -0.07040982693433762, 1.700166940689087, -0.7643944025039673, 179.9066925048828]\n",
      "[340] Loss: 0.005915, Params: [0.11938655376434326, -0.07122238725423813, 1.7001935243606567, -0.7917609810829163, 179.9085235595703]\n",
      "[350] Loss: 0.005913, Params: [0.1194402426481247, -0.07204698026180267, 1.7002193927764893, -0.8195295333862305, 179.9103546142578]\n",
      "[360] Loss: 0.005911, Params: [0.11949340999126434, -0.07288350909948349, 1.7002454996109009, -0.8476936221122742, 179.9121856689453]\n",
      "[370] Loss: 0.005910, Params: [0.11954772472381592, -0.0737314373254776, 1.7002700567245483, -0.8762471675872803, 179.9141082763672]\n",
      "[380] Loss: 0.005908, Params: [0.1196068748831749, -0.07459110021591187, 1.7002917528152466, -0.9051836729049683, 179.9160919189453]\n",
      "[390] Loss: 0.005906, Params: [0.11966390907764435, -0.07546190917491913, 1.7003177404403687, -0.9344974756240845, 179.91807556152344]\n",
      "[400] Loss: 0.005905, Params: [0.1197219267487526, -0.07634323090314865, 1.7003430128097534, -0.9641833305358887, 179.92005920410156]\n",
      "[410] Loss: 0.005903, Params: [0.1197797879576683, -0.07723589986562729, 1.700368046760559, -0.9942350387573242, 179.9220428466797]\n",
      "[420] Loss: 0.005901, Params: [0.11983725428581238, -0.07813914120197296, 1.7003926038742065, -1.02464759349823, 179.9240264892578]\n",
      "[430] Loss: 0.005900, Params: [0.11989527940750122, -0.07905297726392746, 1.7004163265228271, -1.0554156303405762, 179.92605590820312]\n",
      "[440] Loss: 0.005898, Params: [0.11995800584554672, -0.07997742295265198, 1.7004363536834717, -1.086534023284912, 179.92819213867188]\n",
      "[450] Loss: 0.005896, Params: [0.12001975625753403, -0.08091218769550323, 1.7004600763320923, -1.1179978847503662, 179.93032836914062]\n",
      "[460] Loss: 0.005894, Params: [0.12008216232061386, -0.08185654878616333, 1.7004834413528442, -1.1498026847839355, 179.93246459960938]\n",
      "[470] Loss: 0.005892, Params: [0.12014418840408325, -0.08281126618385315, 1.7005071640014648, -1.1819431781768799, 179.93460083007812]\n",
      "[480] Loss: 0.005891, Params: [0.12020636349916458, -0.08377568423748016, 1.7005290985107422, -1.2144150733947754, 179.93673706054688]\n",
      "[490] Loss: 0.005889, Params: [0.12026838213205338, -0.0847499743103981, 1.7005506753921509, -1.2472139596939087, 179.93887329101562]\n",
      "[499] Loss: 0.005887, Params: [0.12032420188188553, -0.0856349840760231, 1.700569987297058, -1.2770087718963623, 179.94082641601562]\n",
      "求解相机参数: [ 1.2032420e-01 -8.5634984e-02  1.7005700e+00 -1.2770088e+00\n",
      "  1.7994083e+02]\n",
      "投影误差 (像素): [204.06226471  87.8194215   70.09765173 143.27714324]\n",
      "平均误差: 126.31 px\n",
      "\n",
      "投影点:\n",
      "[[758.74583435 823.01628685]\n",
      " [487.65951538 853.8944664 ]\n",
      " [650.99493408 421.47505188]\n",
      " [563.24020004 432.24163055]]\n",
      "\n",
      "原始 selected_pts:\n",
      "[[626. 978.]\n",
      " [569. 887.]\n",
      " [636. 353.]\n",
      " [631. 306.]]\n"
     ]
    }
   ],
   "source": [
    "result = build_image_model_dict_screen(input_dir, output_dir)\n",
    "\n",
    "for item in result:\n",
    "    local_pts = item[\"local_pts\"]\n",
    "    with Image.open(item[\"image\"]) as img:\n",
    "        image_width, image_height = img.size\n",
    "    selected_pts = flip_selected_pts_y(item[\"selected_pts\"], image_height)\n",
    "\n",
    "    # 使用 PyTorch 反向传播求相机参数\n",
    "    cam_params = optimize_camera_torch(\n",
    "        local_pts=local_pts,\n",
    "        selected_pts=selected_pts,\n",
    "        image_width=image_width,\n",
    "        image_height=image_height,\n",
    "        fov_deg=60.0,\n",
    "        iterations=500\n",
    "    )\n",
    "    print(\"求解相机参数:\", cam_params)\n",
    "\n",
    "    # 投影并对比\n",
    "    projected_pts = np.array([\n",
    "        project_point(pt3d, cam_params, image_width, image_height)\n",
    "        for pt3d in local_pts\n",
    "    ])\n",
    "    proj_pts_pixel = projected_pts * np.array([image_width, image_height])\n",
    "    selected_pts_pixel = np.array(selected_pts)\n",
    "\n",
    "    errors = np.linalg.norm(proj_pts_pixel - selected_pts_pixel, axis=1)\n",
    "    print(f\"投影误差 (像素): {errors}\")\n",
    "    print(f\"平均误差: {errors.mean():.2f} px\")\n",
    "\n",
    "    print(\"\\n投影点:\")\n",
    "    print(proj_pts_pixel)\n",
    "    print(\"\\n原始 selected_pts:\")\n",
    "    print(selected_pts_pixel)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
