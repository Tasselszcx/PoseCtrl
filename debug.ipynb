{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## unet 2d condition model\n",
    "\n",
    "[unet](https://huggingface.co/docs/diffusers/en/api/models/unet2d-cond)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import AutoencoderKL, DDPMScheduler, UNet2DConditionModel\n",
    "pretrained_model_name_or_path = \"runwayml/stable-diffusion-v1-5\"  # Stable Diffusion v1.5\n",
    "unet = UNet2DConditionModel.from_pretrained(pretrained_model_name_or_path, subfolder=\"unet\")\n",
    "\n",
    "unet_sd = unet.state_dict()\n",
    "# for key, value in unet_sd.items():\n",
    "#     print(key, value.shape)\n",
    "for name in unet.attn_processors.keys():\n",
    "    cross_attention_dim = None if name.endswith(\"attn1.processor\") else unet.config.cross_attention_dim\n",
    "\n",
    "    if name.startswith(\"mid_block\"):\n",
    "        hidden_size = unet.config.block_out_channels[-1]\n",
    "    elif name.startswith(\"up_blocks\"):\n",
    "        block_id = int(name[len(\"up_blocks.\")])\n",
    "        hidden_size = list(reversed(unet.config.block_out_channels))[block_id]\n",
    "    elif name.startswith(\"down_blocks\"):\n",
    "        block_id = int(name[len(\"down_blocks.\")])\n",
    "        hidden_size = unet.config.block_out_channels[block_id]\n",
    "\n",
    "    if cross_attention_dim is None:\n",
    "        pass\n",
    "    else:\n",
    "        layer_name = name.split(\".processor\")[0]\n",
    "        weights = {\n",
    "            \"to_k_ip.weight\": unet_sd[layer_name + \".to_k.weight\"],\n",
    "            \"to_v_ip.weight\": unet_sd[layer_name + \".to_v.weight\"],\n",
    "        }\n",
    "        print(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_k.weight torch.Size([320, 320])\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_v.weight torch.Size([320, 320])\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_k.weight torch.Size([320, 768])\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_v.weight torch.Size([320, 768])\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_k.weight torch.Size([320, 320])\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_v.weight torch.Size([320, 320])\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_k.weight torch.Size([320, 768])\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_v.weight torch.Size([320, 768])\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.weight torch.Size([640, 640])\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.weight torch.Size([640, 640])\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight torch.Size([640, 768])\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight torch.Size([640, 768])\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.weight torch.Size([640, 640])\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.weight torch.Size([640, 640])\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight torch.Size([640, 768])\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight torch.Size([640, 768])\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k.weight torch.Size([1280, 1280])\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v.weight torch.Size([1280, 1280])\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.weight torch.Size([1280, 768])\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.weight torch.Size([1280, 768])\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k.weight torch.Size([1280, 1280])\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v.weight torch.Size([1280, 1280])\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.weight torch.Size([1280, 768])\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.weight torch.Size([1280, 768])\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.weight torch.Size([1280, 1280])\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.weight torch.Size([1280, 1280])\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight torch.Size([1280, 768])\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight torch.Size([1280, 768])\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.weight torch.Size([1280, 1280])\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.weight torch.Size([1280, 1280])\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight torch.Size([1280, 768])\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight torch.Size([1280, 768])\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_k.weight torch.Size([1280, 1280])\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_v.weight torch.Size([1280, 1280])\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_k.weight torch.Size([1280, 768])\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_v.weight torch.Size([1280, 768])\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k.weight torch.Size([640, 640])\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v.weight torch.Size([640, 640])\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.weight torch.Size([640, 768])\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.weight torch.Size([640, 768])\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k.weight torch.Size([640, 640])\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v.weight torch.Size([640, 640])\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.weight torch.Size([640, 768])\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.weight torch.Size([640, 768])\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_k.weight torch.Size([640, 640])\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_v.weight torch.Size([640, 640])\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_k.weight torch.Size([640, 768])\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_v.weight torch.Size([640, 768])\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_k.weight torch.Size([320, 320])\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_v.weight torch.Size([320, 320])\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_k.weight torch.Size([320, 768])\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_v.weight torch.Size([320, 768])\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_k.weight torch.Size([320, 320])\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_v.weight torch.Size([320, 320])\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_k.weight torch.Size([320, 768])\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_v.weight torch.Size([320, 768])\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_k.weight torch.Size([320, 320])\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_v.weight torch.Size([320, 320])\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_k.weight torch.Size([320, 768])\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_v.weight torch.Size([320, 768])\n",
      "mid_block.attentions.0.transformer_blocks.0.attn1.to_k.weight torch.Size([1280, 1280])\n",
      "mid_block.attentions.0.transformer_blocks.0.attn1.to_v.weight torch.Size([1280, 1280])\n",
      "mid_block.attentions.0.transformer_blocks.0.attn2.to_k.weight torch.Size([1280, 768])\n",
      "mid_block.attentions.0.transformer_blocks.0.attn2.to_v.weight torch.Size([1280, 768])\n"
     ]
    }
   ],
   "source": [
    "from diffusers import AutoencoderKL, DDPMScheduler, UNet2DConditionModel\n",
    "pretrained_model_name_or_path = \"runwayml/stable-diffusion-v1-5\"  # Stable Diffusion v1.5\n",
    "unet = UNet2DConditionModel.from_pretrained(pretrained_model_name_or_path, subfolder=\"unet\")\n",
    "for name, param in unet.named_parameters():\n",
    "    if \"to_k\" in name or \"to_v\" in name:\n",
    "        print(name, param.shape)\n",
    "        param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip show torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet.attn_processors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "import json\n",
    "import itertools\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from transformers import CLIPImageProcessor\n",
    "from accelerate import Accelerator\n",
    "from accelerate.logging import get_logger\n",
    "from accelerate.utils import ProjectConfiguration\n",
    "from diffusers import AutoencoderKL, DDPMScheduler, UNet2DConditionModel\n",
    "from transformers import CLIPTextModel, CLIPTokenizer, CLIPVisionModelWithProjection\n",
    "from poseCtrl.models.pose_adaptor import VPmatrixEncoder, VPmatrixPoints\n",
    "from poseCtrl.models.attention_processor import AttnProcessor, PoseAttnProcessor\n",
    "from poseCtrl.data.dataset import MyDataset, load_base_points\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description=\"Simple example of a training script.\")\n",
    "    parser.add_argument(\n",
    "        \"--pretrained_model_name_or_path\",\n",
    "        type=str, \n",
    "        default='runwayml/stable-diffusion-v1-5',\n",
    "        required=True,\n",
    "        help=\"Path to pretrained model or model identifier from huggingface.co/models.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--pretrained_pose_path\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        help=\"Path to pretrained  posectrl model. If not specified weights are initialized randomly.\",\n",
    "    )\n",
    "    # parser.add_argument(\n",
    "    #     \"--data_json_file\",\n",
    "    #     type=str,\n",
    "    #     default=None,\n",
    "    #     required=True,\n",
    "    #     help=\"Training data\",\n",
    "    # )\n",
    "    parser.add_argument(\n",
    "        \"--base_point_path\",\n",
    "        type=str,\n",
    "        default=r'F:\\Projects\\diffusers\\Project\\PoseCtrl\\dataSet\\standardVertex.txt',\n",
    "        help='Path to base model points'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--data_root_path\",\n",
    "        type=str,\n",
    "        default=\"F:\\\\Projects\\\\diffusers\\\\ProgramData\\\\pic\",\n",
    "        required=True,\n",
    "        help=\"Training data root path\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--image_encoder_path\",\n",
    "        type=str,\n",
    "        default=\"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\",\n",
    "        required=True,\n",
    "        help=\"Path to CLIP image encoder\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--output_dir\",\n",
    "        type=str,\n",
    "        default=\"sd-pose_ctrl\",\n",
    "        help=\"The output directory where the model predictions and checkpoints will be written.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--logging_dir\",\n",
    "        type=str,\n",
    "        default=\"logs\",\n",
    "        help=(\n",
    "            \"[TensorBoard](https://www.tensorflow.org/tensorboard) log directory. Will default to\"\n",
    "            \" *output_dir/runs/**CURRENT_DATETIME_HOSTNAME***.\"\n",
    "        ),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--resolution\",\n",
    "        type=int,\n",
    "        default=512,\n",
    "        help=(\n",
    "            \"The resolution for input images\"\n",
    "        ),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--learning_rate\",\n",
    "        type=float,\n",
    "        default=1e-4,\n",
    "        help=\"Learning rate to use.\",\n",
    "    )\n",
    "    parser.add_argument(\"--weight_decay\", type=float, default=1e-2, help=\"Weight decay to use.\")\n",
    "    parser.add_argument(\"--num_train_epochs\", type=int, default=100)\n",
    "    parser.add_argument(\n",
    "        \"--train_batch_size\", type=int, default=8, help=\"Batch size (per device) for the training dataloader.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--dataloader_num_workers\",\n",
    "        type=int,\n",
    "        default=0,\n",
    "        help=(\n",
    "            \"Number of subprocesses to use for data loading. 0 means that the data will be loaded in the main process.\"\n",
    "        ),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--save_steps\",\n",
    "        type=int,\n",
    "        default=2000,\n",
    "        help=(\n",
    "            \"Save a checkpoint of the training state every X updates\"\n",
    "        ),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--mixed_precision\",\n",
    "        type=str,\n",
    "        default=\"fp16\",\n",
    "        choices=[\"no\", \"fp16\", \"bf16\"],\n",
    "        help=(\n",
    "            \"Whether to use mixed precision. Choose between fp16 and bf16 (bfloat16). Bf16 requires PyTorch >=\"\n",
    "            \" 1.10.and an Nvidia Ampere GPU.  Default to the value of accelerate config of the current system or the\"\n",
    "            \" flag passed with the `accelerate.launch` command. Use this argument to override the accelerate config.\"\n",
    "        ),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--report_to\",\n",
    "        type=str,\n",
    "        default=\"tensorboard\",\n",
    "        help=(\n",
    "            'The integration to report the results and logs to. Supported platforms are `\"tensorboard\"`'\n",
    "            ' (default), `\"wandb\"` and `\"comet_ml\"`. Use `\"all\"` to report to all integrations.'\n",
    "        ),\n",
    "    )\n",
    "    parser.add_argument(\"--local_rank\", type=int, default=-1, help=\"For distributed training: local_rank\")\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    env_local_rank = int(os.environ.get(\"LOCAL_RANK\", -1))\n",
    "    if env_local_rank != -1 and env_local_rank != args.local_rank:\n",
    "        args.local_rank = env_local_rank\n",
    "\n",
    "    return args\n",
    "\n",
    "class posectrl(nn.Module):\n",
    "    def __init__(self, unet, vpmatrix_points, atten_modules, ckpt_path=None):\n",
    "        super().__init__()\n",
    "        self.unet = unet\n",
    "        self.vpmatrix_points = vpmatrix_points\n",
    "        self.atten_modules = atten_modules\n",
    "\n",
    "        if ckpt_path is not None:\n",
    "            self.load_from_checkpoint(ckpt_path)\n",
    "\n",
    "    def forward(self, noisy_latents, timesteps, encoder_hidden_states, V_matrix, P_matrix):\n",
    "        point_tokens = self.vpmatrix_points(V_matrix, P_matrix)\n",
    "        \"\"\" 修改:防止之后要加text \"\"\"\n",
    "        if encoder_hidden_states:\n",
    "            encoder_hidden_states = torch.cat([encoder_hidden_states, point_tokens], dim=1)\n",
    "        else:\n",
    "            encoder_hidden_states=point_tokens\n",
    "        # Predict the noise residual\n",
    "        noise_pred = self.unet(noisy_latents, timesteps, encoder_hidden_states).sample\n",
    "        return noise_pred\n",
    "\n",
    "    def load_from_checkpoint(self, ckpt_path: str):\n",
    "        # Calculate original checksums\n",
    "        orig_VPmatrix_sum = torch.sum(torch.stack([torch.sum(p) for p in self.vpmatrix_points.parameters()]))\n",
    "        orig_atten_sum = torch.sum(torch.stack([torch.sum(p) for p in self.atten_modules.parameters()]))\n",
    "\n",
    "        state_dict = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "\n",
    "        # Load state dict for image_proj_model and adapter_modules\n",
    "        self.vpmatrix_points.load_state_dict(state_dict[\"vpmatrix_points\"], strict=True)\n",
    "        self.atten_modules.load_state_dict(state_dict[\"atten_modules\"], strict=True)\n",
    "\n",
    "        # Calculate new checksums\n",
    "        new_VPmatrix_sum = torch.sum(torch.stack([torch.sum(p) for p in self.vpmatrix_points.parameters()]))\n",
    "        new_atten_sum = torch.sum(torch.stack([torch.sum(p) for p in self.atten_modules.parameters()]))\n",
    "\n",
    "        # Verify if the weights have changed\n",
    "        assert orig_VPmatrix_sum != new_VPmatrix_sum, \"Weights of VPmatrixEncoder did not change!\"\n",
    "        assert orig_atten_sum != new_atten_sum, \"Weights of atten_modules did not change!\"\n",
    "\n",
    "        print(f\"Successfully loaded weights from checkpoint {ckpt_path}\")\n",
    "\n",
    "def main():\n",
    "    args = parse_args()\n",
    "    logging_dir = Path(args.output_dir, args.logging_dir)\n",
    "\n",
    "    accelerator_project_config = ProjectConfiguration(project_dir=args.output_dir, logging_dir=logging_dir)\n",
    "\n",
    "    accelerator = Accelerator(\n",
    "        mixed_precision=args.mixed_precision,\n",
    "        log_with=args.report_to,\n",
    "        project_config=accelerator_project_config,\n",
    "    )\n",
    "    \n",
    "    if accelerator.is_main_process:\n",
    "        if args.output_dir is not None:\n",
    "            os.makedirs(args.output_dir, exist_ok=True)\n",
    "\n",
    "    # Load scheduler, tokenizer and models.\n",
    "    noise_scheduler = DDPMScheduler.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"scheduler\")\n",
    "    tokenizer = CLIPTokenizer.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"tokenizer\")\n",
    "    text_encoder = CLIPTextModel.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"text_encoder\")\n",
    "    vae = AutoencoderKL.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"vae\")\n",
    "    unet = UNet2DConditionModel.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"unet\")\n",
    "    image_encoder = CLIPVisionModelWithProjection.from_pretrained(args.image_encoder_path)\n",
    "\n",
    "    # freeze parameters of models to save more memory\n",
    "    unet.requires_grad_(False)\n",
    "    vae.requires_grad_(False)\n",
    "    text_encoder.requires_grad_(False)\n",
    "    image_encoder.requires_grad_(False)\n",
    "    \n",
    "    #vp-matrix encoder\n",
    "    raw_base_points=load_base_points(args.base_point_path)  \n",
    "    vpmatrix_points_sd = VPmatrixEncoder(raw_base_points)\n",
    "\n",
    "    # init pose modules\n",
    "    attn_procs = {}\n",
    "    unet_sd = unet.state_dict()\n",
    "    for name in unet.attn_processors.keys():\n",
    "        cross_attention_dim = None if name.endswith(\"attn1.processor\") else unet.config.cross_attention_dim\n",
    "\n",
    "        if name.startswith(\"mid_block\"):\n",
    "            hidden_size = unet.config.block_out_channels[-1]\n",
    "        elif name.startswith(\"up_blocks\"):\n",
    "            block_id = int(name[len(\"up_blocks.\")])\n",
    "            hidden_size = list(reversed(unet.config.block_out_channels))[block_id]\n",
    "        elif name.startswith(\"down_blocks\"):\n",
    "            block_id = int(name[len(\"down_blocks.\")])\n",
    "            hidden_size = unet.config.block_out_channels[block_id]\n",
    "\n",
    "        if cross_attention_dim is None:\n",
    "            attn_procs[name] = AttnProcessor()\n",
    "        else:\n",
    "            layer_name = name.split(\".processor\")[0]\n",
    "            weights = {\n",
    "                \"to_k_ip.weight\": unet_sd[layer_name + \".to_k.weight\"],\n",
    "                \"to_v_ip.weight\": unet_sd[layer_name + \".to_v.weight\"],\n",
    "            }\n",
    "            attn_procs[name] = PoseAttnProcessor(hidden_size=hidden_size, cross_attention_dim=cross_attention_dim)\n",
    "            attn_procs[name].load_state_dict(weights)\n",
    "\n",
    "    unet.set_attn_processor(attn_procs)\n",
    "\n",
    "    atten_modules = torch.nn.ModuleList(unet.attn_processors.values())\n",
    "    \n",
    "    pose_ctrl = posectrl(unet, vpmatrix_points_sd, atten_modules, args.pretrained_pose_path)\n",
    "    \n",
    "    weight_dtype = torch.float32\n",
    "    if accelerator.mixed_precision == \"fp16\":\n",
    "        weight_dtype = torch.float16\n",
    "    elif accelerator.mixed_precision == \"bf16\":\n",
    "        weight_dtype = torch.bfloat16\n",
    "    #unet.to(accelerator.device, dtype=weight_dtype)\n",
    "    vae.to(accelerator.device, dtype=weight_dtype)\n",
    "    text_encoder.to(accelerator.device, dtype=weight_dtype)\n",
    "    image_encoder.to(accelerator.device, dtype=weight_dtype)\n",
    "    \n",
    "    # optimizer\n",
    "    params_to_opt = itertools.chain(pose_ctrl.vpmatrix_points_sd.parameters(),  pose_ctrl.atten_modules.parameters())\n",
    "    optimizer = torch.optim.AdamW(params_to_opt, lr=args.learning_rate, weight_decay=args.weight_decay)\n",
    "    \n",
    "    # dataloader\n",
    "    train_dataset = MyDataset(args.data_root_path)\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        shuffle=True,\n",
    "        batch_size=args.train_batch_size,\n",
    "        num_workers=args.dataloader_num_workers,\n",
    "    )\n",
    "    \n",
    "    # Prepare everything with our `accelerator`.\n",
    "    pose_ctrl, optimizer, train_dataloader = accelerator.prepare(pose_ctrl, optimizer, train_dataloader)\n",
    "    \n",
    "    global_step = 0\n",
    "    for epoch in range(0, args.num_train_epochs): #default is 100\n",
    "        begin = time.perf_counter()\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            load_data_time = time.perf_counter() - begin\n",
    "            with accelerator.accumulate(pose_ctrl):\n",
    "                # Convert images to latent space\n",
    "                with torch.no_grad():\n",
    "                    latents = vae.encode(batch[\"image\"].to(accelerator.device, dtype=weight_dtype)).latent_dist.sample()\n",
    "                    latents = latents * vae.config.scaling_factor\n",
    "\n",
    "                # Sample noise that we'll add to the latents\n",
    "                noise = torch.randn_like(latents)\n",
    "                bsz = latents.shape[0]\n",
    "                # Sample a random timestep for each image\n",
    "                timesteps = torch.randint(0, noise_scheduler.num_train_timesteps, (bsz,), device=latents.device)\n",
    "                timesteps = timesteps.long()\n",
    "\n",
    "                # Add noise to the latents according to the noise magnitude at each timestep\n",
    "                # (this is the forward diffusion process)\n",
    "                noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "\n",
    "                if \"text_input_ids\" in batch:\n",
    "                    with torch.no_grad():\n",
    "                        encoder_hidden_states = text_encoder(batch[\"text_input_ids\"].to(accelerator.device))[0]\n",
    "                else:\n",
    "                    encoder_hidden_states=None\n",
    "                \n",
    "                noise_pred = pose_ctrl(noisy_latents, timesteps, encoder_hidden_states, batch['view_matrix'], batch['projection_matrix'])\n",
    "        \n",
    "                loss = F.mse_loss(noise_pred.float(), noise.float(), reduction=\"mean\")\n",
    "            \n",
    "                # Gather the losses across all processes for logging (if we use distributed training).\n",
    "                avg_loss = accelerator.gather(loss.repeat(args.train_batch_size)).mean().item()\n",
    "                \n",
    "                # Backpropagate\n",
    "                accelerator.backward(loss)\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                if accelerator.is_main_process:\n",
    "                    print(\"Epoch {}, step {}, data_time: {}, time: {}, step_loss: {}\".format(\n",
    "                        epoch, step, load_data_time, time.perf_counter() - begin, avg_loss))\n",
    "            \n",
    "            global_step += 1\n",
    "            \n",
    "            if global_step % args.save_steps == 0:\n",
    "                save_path = os.path.join(args.output_dir, f\"checkpoint-{global_step}\")\n",
    "                accelerator.save_state(save_path)\n",
    "            \n",
    "            begin = time.perf_counter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "里面结构没改\n",
    "\n",
    "应该有两个输入：图像和VP\n",
    "\n",
    "VP： VPmatrixEncoder -> [77,77]\n",
    "\n",
    "image: \n",
    "- :vae ->latent\n",
    "- :resampler? 写一个 vit或者别的网络\n",
    "- 那么训练参数会变成三个\n",
    "\n",
    "\n",
    "# TODO\n",
    "- 1. visEncoder.py\n",
    "- 2. attention_processor.py： 加逻辑\n",
    "- 3. posectrl.py: 加参数和逻辑\n",
    "- 4. main.py：加参数\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "ckpt = \"checkpoint-50000/pytorch_model.bin\"\n",
    "sd = torch.load(ckpt, map_location=\"cpu\")\n",
    "VPmatrixEncoder_sd = {}\n",
    "atten_sd = {}\n",
    "for k in sd:\n",
    "    if k.startswith(\"unet\"):\n",
    "        pass\n",
    "    elif k.startswith(\"VPmatrixEncoder\"):\n",
    "        VPmatrixEncoder_sd[k.replace(\"VPmatrixEncoder.\", \"\")] = sd[k]\n",
    "    elif k.startswith(\"atten_modules\"):\n",
    "        atten_sd[k.replace(\"atten_modules.\", \"\")] = sd[k]\n",
    "\n",
    "torch.save({\"VPmatrixEncoder\": VPmatrixEncoder_sd, \"atten_modules\": atten_sd}, \"posectrl.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 位置矩阵\n",
    "2. 本地的坐标：可以和vp矩阵相乘，数学意义，M矩阵，\n",
    "3. 多样性一点：图的特征，加上正面原图随便的特征。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "4. 反向：训练什么"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO NEW Version 1\n",
    "\n",
    "- inference\n",
    "- VP矩阵不需要处理了\n",
    "- BasePoints: [2000,4,4] @ [4,4] -> <77 768>（这个是text attention之后的结果，不知道图片他们都是怎么做的）可能可以换个大小，可学习的部分直接写出来更换就行。\n",
    "- 好像流程就没啥问题了\n",
    "- 每个要改的地方加上\"修改\",不然找不到忘记了.\n",
    "\n",
    "# TODO NEW Version 2\n",
    "   现在的逻辑是： vp矩阵[4,4], 顶点是[13860,4] (77x180), ->[13840,4] reshape [77,768]\n",
    "-  要改：\n",
    "   \n",
    "~~- base_load~~\n",
    "\n",
    "~~- pose_adaptor~~\n",
    "\n",
    "   ~~- posectrl traning~~ \n",
    "   - 和 posectrl inference   \n",
    " \n",
    "   ~~- attention_pocessor~~ 和之前没区别\n",
    "\n",
    "   ~~- train main~~\n",
    "\n",
    "   ~~- 跑通~~\n",
    "\n",
    "# TODO NEW Version 3\n",
    "加了个参考图， 这个图attention 加上\n",
    "\n",
    "~~- dataset 1024 resize~~, 可以不加数量限制\n",
    "\n",
    "~~- image sampler~~\n",
    "\n",
    "~~- posectrl train main~~\n",
    "\n",
    "~~- attention~~\n",
    "\n",
    "~~- posectrl.py~~\n",
    "\n",
    "~~- inference~~\n",
    "\n",
    "~~- weights new version~~\n",
    "\n",
    "~~- download ~~\n",
    "\n",
    "  ~~- check in inference and normal pipeline~~、\n",
    "  \n",
    "  ~~- upload to google drive~~\n",
    "\n",
    "- validaton\n",
    "- \n",
    "~~- 检查一遍整个流程~~\n",
    "- train 得到weights\n",
    "- inference\n",
    "\n",
    "# TODO NEW Version 4\n",
    "- 如果把输入引入噪声里面比单纯引入attention会好一点吗\n",
    "- animediff是使用attention的改变了吗\n",
    "- 如果通过lora去训练模型，怎么实现\n",
    "- 如果加上我的模型做lora，会不会直接省了我的事情\n",
    "\n",
    "\n",
    "# FUTURE VERSION\n",
    "- add lora\n",
    "- add 正负判断\n",
    "\n",
    "# Questions\n",
    "~~1. 需不要把好坏prompt设置成~~\n",
    " \n",
    "python \n",
    "```\n",
    "if prompt is None:\n",
    "    prompt = \"best quality, high quality\"\n",
    "if negative_prompt is None:\n",
    "    negative_prompt = \"monochrome, lowres, bad anatomy, worst quality, low quality\"\n",
    "```\n",
    "\n",
    "不用应该\n",
    "\n",
    "- 2. point-e好像是生成3d底模的东西,不知道有没有用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "\n",
    "# 1. 选择 CLIP 预训练模型\n",
    "model_name = \"openai/clip-vit-base-patch16\"  # 也可以换成 \"openai/clip-vit-large-patch14\"\n",
    "model = CLIPModel.from_pretrained(model_name)\n",
    "processor = CLIPProcessor.from_pretrained(model_name)\n",
    "\n",
    "# 2. 读取并预处理图片\n",
    "image_path = r\"F:\\Projects\\diffusers\\ProgramData\\sample_new\\NPC_Avatar_Girl_Sword_Ayaka\\feature.png\"  # 替换成你的图片路径\n",
    "image = Image.open(image_path)\n",
    "\n",
    "inputs = processor(images=image, return_tensors=\"pt\")  # 预处理\n",
    "image_tensor = inputs[\"pixel_values\"]  # 获取输入张量，形状 (1, 3, 224, 224)\n",
    "\n",
    "# 3. 获取所有 patch 的特征\n",
    "with torch.no_grad():\n",
    "    vision_outputs = model.vision_model(image_tensor)  # 获取所有 Transformer 层的输出\n",
    "    patch_features = vision_outputs.last_hidden_state  # 形状: (B, X+1, 768)\n",
    "\n",
    "# 4. 移除 CLS token（第一个 token）\n",
    "patch_features = patch_features[:, 1:, :]  # (B, X, 768)\n",
    "\n",
    "print(\"Patch feature shape:\", patch_features.shape)  # 目标形状: (B, X, 768)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch\n",
    "from pathlib import Path\n",
    "def change_checkpoint(checkpoint_path, new_checkpoint_path):\n",
    "    sd = torch.load(checkpoint_path, map_location=\"cpu\")\n",
    "    vpmatrix_points_sd = {}\n",
    "    atten_sd = {}\n",
    "    proj_sd={}\n",
    "    for k in sd:\n",
    "        if k.startswith(\"unet\"):\n",
    "            pass\n",
    "        elif k.startswith(\"vpmatrix_points\"):\n",
    "            vpmatrix_points_sd[k.replace(\"vpmatrix_points.\", \"\")] = sd[k]\n",
    "        elif k.startswith(\"atten_modules\"):\n",
    "            atten_sd[k.replace(\"atten_modules.\", \"\")] = sd[k]\n",
    "        elif k.startswith(\"image_proj_model\"):\n",
    "            proj_sd[k.replace(\"image_proj_model.\", \"\")] = sd[k]\n",
    "    new_checkpoint_path = Path(new_checkpoint_path, \"posectrl.bin\")\n",
    "    print(vpmatrix_points_sd)\n",
    "    print(atten_sd)\n",
    "    print(proj_sd)\n",
    "    for name in sd['state'].keys():\n",
    "        print(name)\n",
    "    torch.save({\"vpmatrix_points\": vpmatrix_points_sd, \"atten_modules\": atten_sd, \"image_proj_model\": proj_sd}, new_checkpoint_path)\n",
    "    print(f\"Saved new checkpoint to {new_checkpoint_path}\")\n",
    "\n",
    "ckpt = r\"F:\\Projects\\diffusers\\Project\\PoseCtrl\\sd-pose_ctrl\\model.safetensors\"\n",
    "\n",
    "change_checkpoint(ckpt, r\"F:\\Projects\\diffusers\\Project\\PoseCtrl\\sd-pose_ctrl\\transfer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from safetensors.torch import load_file\n",
    "from pathlib import Path\n",
    "\n",
    "def change_checkpoint(checkpoint_path, new_checkpoint_path):\n",
    "    # 使用 safetensors 加载文件\n",
    "    sd = load_file(checkpoint_path)\n",
    "    \n",
    "    vpmatrix_points_sd = {}\n",
    "    atten_sd = {}\n",
    "    proj_sd = {}\n",
    "    \n",
    "    # 遍历模型权重并分类\n",
    "    i, j = 1 , 1\n",
    "    for k in sd:\n",
    "        # print(k)\n",
    "        if k.startswith(\"unet\"):\n",
    "            pass\n",
    "        elif k.startswith(\"vpmatrix_points\"):\n",
    "            vpmatrix_points_sd[k.replace(\"vpmatrix_points.\", \"\")] = sd[k]\n",
    "        elif k.startswith(\"atten_modules\"):\n",
    "            atten_sd[k.replace(\"atten_modules.\", \"\")] = sd[k]\n",
    "        elif k.startswith(\"image_proj_model\"):\n",
    "            proj_sd[k.replace(\"image_proj_model.\", \"\")] = sd[k]\n",
    "\n",
    "    # 指定新的文件路径\n",
    "    new_checkpoint_path = Path(new_checkpoint_path, \"posectrl2.bin\")\n",
    "    \n",
    "    print(atten_sd)\n",
    "    \n",
    "    # 保存为新的二进制 checkpoint 文件\n",
    "    torch.save({\n",
    "        \"vpmatrix_points\": vpmatrix_points_sd,\n",
    "        \"atten_modules\": atten_sd,\n",
    "        \"image_proj_model\": proj_sd\n",
    "    }, new_checkpoint_path)\n",
    "    \n",
    "    print(f\"Saved new checkpoint to {new_checkpoint_path}\")\n",
    "\n",
    "# 使用 safetensors 文件路径\n",
    "ckpt = r\"F:\\Projects\\diffusers\\Project\\PoseCtrl\\sd-pose_ctrl\\model.safetensors\"\n",
    "\n",
    "# 调用转换函数\n",
    "change_checkpoint(ckpt, r\"F:\\Projects\\diffusers\\Project\\PoseCtrl\\sd-pose_ctrl\\transfer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch\n",
    "from diffusers import StableDiffusionPipeline, StableDiffusionImg2ImgPipeline, StableDiffusionInpaintPipelineLegacy, DDIMScheduler, AutoencoderKL\n",
    "from PIL import Image\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(r'F:\\Projects\\diffusers\\Project\\PoseCtrl')\n",
    "sys.path.append(r'F:\\Projects\\diffusers\\Project\\PoseCtrl\\poseCtrl')\n",
    "from poseCtrl.models.pose_adaptor import VPmatrixPoints, ImageProjModel\n",
    "from poseCtrl.models.attention_processor import AttnProcessor, PoseAttnProcessor\n",
    "from poseCtrl.data.dataset import CustomDataset, load_base_points\n",
    "from poseCtrl.models.posectrl import PoseCtrl\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "base_point_path=r'F:\\Projects\\diffusers\\Project\\PoseCtrl\\dataSet\\standardVertex.txt'\n",
    "raw_base_points=load_base_points(base_point_path)  \n",
    "\n",
    "base_model_path = \"runwayml/stable-diffusion-v1-5\"\n",
    "vae_model_path = \"stabilityai/sd-vae-ft-mse\"\n",
    "image_encoder_path = \"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\"\n",
    "ip_ckpt = r\"F:\\Projects\\diffusers\\Project\\sd-pose_ctrl\\trail_3\\posectrl2000.bin\"\n",
    "device = \"cuda\"\n",
    "\n",
    "def image_grid(imgs, rows, cols):\n",
    "    assert len(imgs) == rows*cols\n",
    "\n",
    "    w, h = imgs[0].size\n",
    "    grid = Image.new('RGB', size=(cols*w, rows*h))\n",
    "    grid_w, grid_h = grid.size\n",
    "    \n",
    "    for i, img in enumerate(imgs):\n",
    "        grid.paste(img, box=(i%cols*w, i//cols*h))\n",
    "    return grid\n",
    "\n",
    "noise_scheduler = DDIMScheduler(\n",
    "    num_train_timesteps=1000,\n",
    "    beta_start=0.00085,\n",
    "    beta_end=0.012,\n",
    "    beta_schedule=\"scaled_linear\",\n",
    "    clip_sample=False,\n",
    "    set_alpha_to_one=False,\n",
    "    steps_offset=1,\n",
    ")\n",
    "vae = AutoencoderKL.from_pretrained(vae_model_path).to(dtype=torch.float16)\n",
    "\n",
    "# load SD pipeline\n",
    "pipe = StableDiffusionImg2ImgPipeline.from_pretrained(\n",
    "    base_model_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    scheduler=noise_scheduler,\n",
    "    vae=vae,\n",
    "    feature_extractor=None,\n",
    "    safety_checker=None\n",
    ")\n",
    "\n",
    "path = r\"F:\\\\Projects\\\\diffusers\\\\ProgramData\\\\sample_new\"\n",
    "dataset = CustomDataset(path)\n",
    "data = dataset[0]\n",
    "from torchvision import transforms\n",
    "\n",
    "transform = transforms.Resize((256, 256))\n",
    "\n",
    "image = data['image']\n",
    "image = transform(image) \n",
    "\n",
    "image_np = image.permute(1, 2, 0).cpu().numpy()\n",
    "image_np = (image_np * 255).astype(np.uint8)\n",
    "g_image = data['feature']\n",
    "g_image = transform(g_image) \n",
    "\n",
    "g_image_np = g_image.permute(1, 2, 0).cpu().numpy()\n",
    "g_image_np = (g_image_np * 255).astype(np.uint8)\n",
    "vmatrix = data['view_matrix'].to(torch.float16).unsqueeze(0).to(device)\n",
    "pmatrix = data['projection_matrix'].to(torch.float16).unsqueeze(0).to(device)\n",
    "\n",
    "pose_model = PoseCtrl(pipe, image_encoder_path, ip_ckpt, raw_base_points, device)\n",
    "images = pose_model.generate(pil_image=image_np, num_samples=4, num_inference_steps=50, seed=42, image=g_image_np, strength=0.6, V_matrix=vmatrix,P_matrix=pmatrix )\n",
    "grid = image_grid(images, 1, 4)\n",
    "grid\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pose_model = PoseCtrl(pipe, image_encoder_path, ip_ckpt, raw_base_points, device)\n",
    "images = pose_model.generate(pil_image=image_np, num_samples=4, num_inference_steps=50, seed=42, image=g_image_np, strength=0.6, V_matrix=vmatrix,P_matrix=pmatrix )\n",
    "grid = image_grid(images, 1, 4)\n",
    "grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "image = data['image']\n",
    "# 将 (C, H, W) 转换为 (H, W, C) 并转换为 NumPy\n",
    "image_np = image.permute(1, 2, 0).cpu().numpy()\n",
    "image_np = (image_np * 255).astype(np.uint8)\n",
    "# 显示图像\n",
    "plt.imshow(image_np)\n",
    "plt.axis('off')  # 不显示坐标轴\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from safetensors.torch import load_file\n",
    "import pickle\n",
    "\n",
    "def convert_sd_weights_to_bin(folder_path, output_bin_path):\n",
    "    \"\"\"\n",
    "    将 Stable Diffusion 文件夹中的多个权重文件（safetensors, bin, pkl, pt）合并并保存为 .bin 格式。\n",
    "\n",
    "    :param folder_path: 包含权重文件的文件夹路径\n",
    "    :param output_bin_path: 输出的 .bin 文件路径\n",
    "    \"\"\"\n",
    "    merged_state_dict = {}  # 存储所有权重的字典\n",
    "\n",
    "    # 遍历文件夹，找到所有权重文件\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        \n",
    "        if filename.endswith('.safetensors'):\n",
    "            print(f\"Loading {file_path} (safetensors)...\")\n",
    "            state_dict = load_file(file_path, device=\"cpu\")\n",
    "        elif filename.endswith('.bin'):\n",
    "            print(f\"Loading {file_path} (bin)...\")\n",
    "            state_dict = torch.load(file_path, map_location=\"cpu\")\n",
    "        elif filename.endswith('.pkl'):\n",
    "            print(f\"Loading {file_path} (pkl)...\")\n",
    "            with open(file_path, \"rb\") as f:\n",
    "                state_dict = pickle.load(f)\n",
    "        elif filename.endswith('.pt'):\n",
    "            print(f\"Loading {file_path} (pt)...\")\n",
    "            state_dict = torch.load(file_path, map_location=\"cpu\")\n",
    "        else:\n",
    "            print(f\"Skipping {file_path}, unsupported format.\")\n",
    "            continue\n",
    "\n",
    "        # 合并权重（如果存在相同的 key，则不会覆盖）\n",
    "        for key, value in state_dict.items():\n",
    "            if key not in merged_state_dict:\n",
    "                merged_state_dict[key] = value\n",
    "\n",
    "    # 保存到 .bin 文件\n",
    "    torch.save(merged_state_dict, output_bin_path)\n",
    "    print(f\"Saved merged weights to {output_bin_path}\")\n",
    "\n",
    "# 示例用法\n",
    "folder_path = r\"/content/sd-pose_ctrl/checkpoint-20\"  # 替换为你的文件夹路径\n",
    "output_bin_path = r\"/content/sd-pose_ctrl/checkpoint-20/stable_diffusion.bin\"  # 目标 bin 文件路径\n",
    "\n",
    "convert_sd_weights_to_bin(folder_path, output_bin_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install omegaconf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionPipeline\n",
    "\n",
    "# 加载 .safetensors 文件\n",
    "pipeline = StableDiffusionPipeline.from_single_file(r\"F:\\Projects\\diffusers\\ProgramData\\tmndMix_tmndMixSPRAINBOW.safetensors\")\n",
    "\n",
    "# 将模型保存为 diffusers 格式\n",
    "pipeline.save_pretrained(r\"F:\\Projects\\diffusers\\ProgramData\\basemodel\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch\n",
    "from diffusers import StableDiffusionPipeline, StableDiffusionImg2ImgPipeline, StableDiffusionInpaintPipelineLegacy, DDIMScheduler, AutoencoderKL\n",
    "from PIL import Image\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(r'F:\\Projects\\diffusers\\Project\\PoseCtrl')\n",
    "sys.path.append(r'F:\\Projects\\diffusers\\Project\\PoseCtrl\\poseCtrl')\n",
    "from poseCtrl.models.pose_adaptor import VPmatrixPoints, ImageProjModel\n",
    "from poseCtrl.models.attention_processor import AttnProcessor, PoseAttnProcessor\n",
    "from poseCtrl.data.dataset import CustomDataset, load_base_points\n",
    "from poseCtrl.models.posectrl import PoseCtrl\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "base_point_path=r'F:\\Projects\\diffusers\\Project\\PoseCtrl\\dataSet\\standardVertex.txt'\n",
    "raw_base_points=load_base_points(base_point_path)  \n",
    "\n",
    "base_model_path = r\"F:\\Projects\\diffusers\\ProgramData\\basemodel\"\n",
    "vae_model_path = \"stabilityai/sd-vae-ft-mse\"\n",
    "image_encoder_path = \"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\"\n",
    "ip_ckpt = r\"F:\\Projects\\diffusers\\Project\\sd-pose_ctrl\\trail_1\\posectrl.bin\"\n",
    "device = \"cuda\"\n",
    "\n",
    "def image_grid(imgs, rows, cols):\n",
    "    assert len(imgs) == rows*cols\n",
    "\n",
    "    w, h = imgs[0].size\n",
    "    grid = Image.new('RGB', size=(cols*w, rows*h))\n",
    "    grid_w, grid_h = grid.size\n",
    "    \n",
    "    for i, img in enumerate(imgs):\n",
    "        grid.paste(img, box=(i%cols*w, i//cols*h))\n",
    "    return grid\n",
    "\n",
    "noise_scheduler = DDIMScheduler(\n",
    "    num_train_timesteps=1000,\n",
    "    beta_start=0.00085,\n",
    "    beta_end=0.012,\n",
    "    beta_schedule=\"scaled_linear\",\n",
    "    clip_sample=False,\n",
    "    set_alpha_to_one=False,\n",
    "    steps_offset=1,\n",
    ")\n",
    "vae = AutoencoderKL.from_pretrained(vae_model_path).to(dtype=torch.float16)\n",
    "\n",
    "# load SD pipeline\n",
    "pipe = StableDiffusionImg2ImgPipeline.from_pretrained(\n",
    "    base_model_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    scheduler=noise_scheduler,\n",
    "    vae=vae,\n",
    "    feature_extractor=None,\n",
    "    safety_checker=None\n",
    ")\n",
    "\n",
    "path = r\"F:\\\\Projects\\\\diffusers\\\\ProgramData\\\\sample_new\"\n",
    "dataset = CustomDataset(path)\n",
    "data = dataset[0]\n",
    "from torchvision import transforms\n",
    "\n",
    "transform = transforms.Resize((256, 256))\n",
    "\n",
    "\n",
    "image = data['image']\n",
    "image_pil = transforms.ToPILImage()(image)\n",
    "image_pil = transform(image_pil) \n",
    "image = Image.open('F:\\Projects\\diffusers\\Project\\PoseCtrl\\image.jpg').convert('RGB')\n",
    "g_image = data['feature']\n",
    "g_image_pil = transforms.ToPILImage()(g_image)\n",
    "g_image_pil = transform(g_image_pil) \n",
    "\n",
    "vmatrix = data['view_matrix'].to(torch.float16).unsqueeze(0).to(device)\n",
    "pmatrix = data['projection_matrix'].to(torch.float16).unsqueeze(0).to(device)\n",
    "\n",
    "pose_model = PoseCtrl(pipe, image_encoder_path, ip_ckpt, raw_base_points, device)\n",
    "images = pose_model.generate(pil_image=g_image_pil, num_samples=4, num_inference_steps=50, seed=37, image=image, strength=0.6, V_matrix=vmatrix,P_matrix=pmatrix )\n",
    "# images = pose_model.generate(pil_image=g_image, num_samples=4, num_inference_steps=50, seed=42, strength=0.6, V_matrix=vmatrix,P_matrix=pmatrix )\n",
    "grid = image_grid(images, 1, 4)\n",
    "grid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_pil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = pose_model.generate(pil_image=g_image, num_samples=4, num_inference_steps=50, seed=42, image=image, strength=0.6, V_matrix=vmatrix,P_matrix=pmatrix )\n",
    "# images = pose_model.generate(pil_image=g_image, num_samples=4, num_inference_steps=50, seed=42, strength=0.6, V_matrix=vmatrix,P_matrix=pmatrix )\n",
    "grid = image_grid(images, 1, 4)\n",
    "grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_pil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_pil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pose_model = PoseCtrl(pipe, image_encoder_path, ip_ckpt, raw_base_points, device)\n",
    "images = pose_model.generate(pil_image=g_image, num_samples=4, num_inference_steps=50, seed=42, image=image, strength=0.6, V_matrix=vmatrix,P_matrix=pmatrix )\n",
    "# images = pose_model.generate(pil_image=g_image, num_samples=4, num_inference_steps=50, seed=42, strength=0.6, V_matrix=vmatrix,P_matrix=pmatrix )\n",
    "grid = image_grid(images, 1, 4)\n",
    "grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_pil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = pose_model.generate(pil_image=g_image_pil, num_samples=4, num_inference_steps=100, seed=42, image=image_pil, strength=0.5, V_matrix=vmatrix,P_matrix=pmatrix )\n",
    "# images = pose_model.generate(pil_image=g_image, num_samples=4, num_inference_steps=50, seed=42, strength=0.6, V_matrix=vmatrix,P_matrix=pmatrix )\n",
    "grid = image_grid(images, 1, 4)\n",
    "grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"F:\\Projects\\diffusers\\ProgramData\\validation\"\n",
    "dataset = CustomDataset(path)\n",
    "data = dataset[56]\n",
    "from torchvision import transforms\n",
    "\n",
    "transform = transforms.Resize((256, 256))\n",
    "\n",
    "\n",
    "image = data['image']\n",
    "image_pil = transforms.ToPILImage()(image)\n",
    "image_pil = transform(image_pil) \n",
    "\n",
    "g_image = data['feature']\n",
    "g_image_pil = transforms.ToPILImage()(g_image)\n",
    "g_image_pil = transform(g_image_pil) \n",
    "\n",
    "vmatrix = data['view_matrix'].to(torch.float16).unsqueeze(0).to(device)\n",
    "pmatrix = data['projection_matrix'].to(torch.float16).unsqueeze(0).to(device)\n",
    "image_pil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = pose_model.generate(pil_image=g_image, num_samples=4, num_inference_steps=50, seed=42,image=image, strength=0.6, V_matrix=vmatrix,P_matrix=pmatrix )\n",
    "grid = image_grid(images, 1, 4)\n",
    "grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch\n",
    "from diffusers import StableDiffusionPipeline, StableDiffusionImg2ImgPipeline, StableDiffusionInpaintPipelineLegacy, DDIMScheduler, AutoencoderKL\n",
    "from PIL import Image\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(r'F:\\Projects\\diffusers\\Project\\PoseCtrl')\n",
    "sys.path.append(r'F:\\Projects\\diffusers\\Project\\PoseCtrl\\poseCtrl')\n",
    "from poseCtrl.models.pose_adaptor import VPmatrixPoints, ImageProjModel\n",
    "from poseCtrl.models.attention_processor import AttnProcessor, PoseAttnProcessor\n",
    "from poseCtrl.data.dataset import CustomDataset, load_base_points\n",
    "from poseCtrl.models.posectrl import PoseCtrl\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "base_point_path=r'F:\\Projects\\diffusers\\Project\\PoseCtrl\\dataSet\\standardVertex.txt'\n",
    "raw_base_points=load_base_points(base_point_path)  \n",
    "\n",
    "base_model_path = r\"F:\\Projects\\diffusers\\ProgramData\\basemodel\"\n",
    "vae_model_path = \"stabilityai/sd-vae-ft-mse\"\n",
    "image_encoder_path = \"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\"\n",
    "ip_ckpt = r\"F:\\Projects\\diffusers\\Project\\sd-pose_ctrl\\trail_4\\posectrl.bin\"\n",
    "device = \"cuda\"\n",
    "\n",
    "def image_grid(imgs, rows, cols):\n",
    "    assert len(imgs) == rows*cols\n",
    "\n",
    "    w, h = imgs[0].size\n",
    "    grid = Image.new('RGB', size=(cols*w, rows*h))\n",
    "    grid_w, grid_h = grid.size\n",
    "    \n",
    "    for i, img in enumerate(imgs):\n",
    "        grid.paste(img, box=(i%cols*w, i//cols*h))\n",
    "    return grid\n",
    "\n",
    "noise_scheduler = DDIMScheduler(\n",
    "    num_train_timesteps=1000,\n",
    "    beta_start=0.00085,\n",
    "    beta_end=0.012,\n",
    "    beta_schedule=\"scaled_linear\",\n",
    "    clip_sample=False,\n",
    "    set_alpha_to_one=False,\n",
    "    steps_offset=1,\n",
    ")\n",
    "vae = AutoencoderKL.from_pretrained(vae_model_path).to(dtype=torch.float16)\n",
    "\n",
    "# load SD pipeline\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    base_model_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    scheduler=noise_scheduler,\n",
    "    vae=vae,\n",
    "    feature_extractor=None,\n",
    "    safety_checker=None\n",
    ")\n",
    "path = r\"F:\\\\Projects\\\\diffusers\\\\ProgramData\\\\sample_new\"\n",
    "dataset = CustomDataset(path)\n",
    "data = dataset[344]\n",
    "from torchvision import transforms\n",
    "\n",
    "transform = transforms.Resize((256, 256))\n",
    "\n",
    "\n",
    "image = data['image']\n",
    "image_pil = transforms.ToPILImage()(image)\n",
    "image_pil = transform(image_pil)  \n",
    "\n",
    "g_image = data['feature']\n",
    "g_image_pil = transforms.ToPILImage()(g_image)\n",
    "g_image_pil = transform(g_image_pil) \n",
    "\n",
    "vmatrix = data['view_matrix'].to(torch.float16).unsqueeze(0).to(device)\n",
    "pmatrix = data['projection_matrix'].to(torch.float16).unsqueeze(0).to(device)\n",
    "\n",
    "pose_model = PoseCtrl(pipe, image_encoder_path, ip_ckpt, raw_base_points, device)\n",
    "# images = pose_model.generate(pil_image=g_image, num_samples=4, num_inference_steps=50, seed=42, image=image, strength=0.6, V_matrix=vmatrix,P_matrix=pmatrix )\n",
    "images = pose_model.generate(pil_image=g_image, num_samples=4, num_inference_steps=100, seed=41, strength=0.6, V_matrix=vmatrix,P_matrix=pmatrix )\n",
    "grid = image_grid(images, 1, 4)\n",
    "grid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = pose_model.generate(pil_image=g_image, num_samples=4, num_inference_steps=10, seed=41, strength=0.6, V_matrix=vmatrix,P_matrix=pmatrix )\n",
    "grid = image_grid(images, 1, 4)\n",
    "grid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_image_pil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pose_model = PoseCtrl(pipe, image_encoder_path, ip_ckpt, raw_base_points, device)\n",
    "# images = pose_model.generate(pil_image=g_image, num_samples=4, num_inference_steps=50, seed=42, image=image, strength=0.6, V_matrix=vmatrix,P_matrix=pmatrix )\n",
    "images = pose_model.generate(pil_image=g_image, num_samples=4, num_inference_steps=50, seed=41, strength=0.6, V_matrix=vmatrix,P_matrix=pmatrix )\n",
    "grid = image_grid(images, 1, 4)\n",
    "grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_pil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset[125]\n",
    "from torchvision import transforms\n",
    "\n",
    "transform = transforms.Resize((256, 256))\n",
    "\n",
    "\n",
    "image = data['image']\n",
    "image_pil = transforms.ToPILImage()(image)\n",
    "image_pil = transform(image_pil) \n",
    "\n",
    "g_image = data['feature']\n",
    "g_image_pil = transforms.ToPILImage()(g_image)\n",
    "g_image_pil = transform(g_image_pil) \n",
    "\n",
    "vmatrix = data['view_matrix'].to(torch.float16).unsqueeze(0).to(device)\n",
    "pmatrix = data['projection_matrix'].to(torch.float16).unsqueeze(0).to(device)\n",
    "\n",
    "# images = pose_model.generate(pil_image=g_image, num_samples=4, num_inference_steps=50, seed=42, image=image, strength=0.6, V_matrix=vmatrix,P_matrix=pmatrix )\n",
    "images = pose_model.generate(pil_image=g_image, num_samples=4, num_inference_steps=50, seed=42, strength=0.6, V_matrix=vmatrix,P_matrix=pmatrix )\n",
    "grid = image_grid(images, 1, 4)\n",
    "grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_pil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(r\"F:\\Projects\\diffusers\\ProgramData\\validation\\NPC_Homeworld_Avatar_Loli_Catalyst_Klee_Edit\\capture_131.png\").convert(\"RGB\")\n",
    "\n",
    "g_image_pil = Image.open(r\"F:\\Projects\\diffusers\\ProgramData\\validation\\NPC_Homeworld_Avatar_Loli_Catalyst_Klee_Edit\\feature.png\")\n",
    "\n",
    "images = pose_model.generate(pil_image=g_image_pil, num_samples=4, num_inference_steps=30, seed=42,image=image, strength=0.6, V_matrix=vmatrix,P_matrix=pmatrix )\n",
    "grid = image_grid(images, 1, 4)\n",
    "grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = pose_model.generate(pil_image=g_image_pil, num_samples=4, num_inference_steps=30, seed=42, strength=0.6, V_matrix=vmatrix,P_matrix=pmatrix )\n",
    "grid = image_grid(images, 1, 4)\n",
    "grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_pil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images = pose_model.generate(pil_image=image, num_samples=4, num_inference_steps=50, seed=42, image=image, strength=0.6, V_matrix=vmatrix,P_matrix=pmatrix )\n",
    "images = pose_model.generate(pil_image=g_image, num_samples=4, num_inference_steps=50, seed=42, image=image, strength=0.6, V_matrix=vmatrix,P_matrix=pmatrix )\n",
    "grid = image_grid(images, 1, 4)\n",
    "grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images = pose_model.generate(pil_image=image, num_samples=4, num_inference_steps=50, seed=42, image=image, strength=0.6, V_matrix=vmatrix,P_matrix=pmatrix )\n",
    "images = pose_model.generate(pil_image=g_image, num_samples=4, num_inference_steps=50, seed=42, image=image, strength=0.6, V_matrix=vmatrix,P_matrix=pmatrix )\n",
    "grid = image_grid(images, 1, 4)\n",
    "grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_pil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images = pose_model.generate(pil_image=image, num_samples=4, num_inference_steps=50, seed=42, image=image, strength=0.6, V_matrix=vmatrix,P_matrix=pmatrix )\n",
    "images = pose_model.generate(pil_image=g_image_pil, num_samples=4, num_inference_steps=50, seed=42, image=image_pil, strength=0.6, V_matrix=vmatrix,P_matrix=pmatrix )\n",
    "grid = image_grid(images, 1, 4)\n",
    "grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_pil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 加载 bin checkpoint 文件\n",
    "checkpoint = torch.load(r\"F:\\Projects\\diffusers\\Project\\sd-pose_ctrl\\posectrl.bin\", map_location=\"cpu\")\n",
    "\n",
    "# 查看 keys（通常包含 'model_state_dict', 'optimizer_state_dict' 等）\n",
    "print(checkpoint.keys())\n",
    "\n",
    "# 进一步查看 model_state_dict 的参数\n",
    "if \"image_proj_model\" in checkpoint:\n",
    "    print(checkpoint[\"image_proj_model\"].keys())\n",
    "    print(checkpoint[\"image_proj_model\"].values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPTextModel, CLIPTokenizer, CLIPVisionModelWithProjection, CLIPProcessor\n",
    "processor = CLIPProcessor.from_pretrained(\"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\")\n",
    "image_encoder = CLIPVisionModelWithProjection.from_pretrained(image_encoder_path).to(device)\n",
    "# inputs = processor(images=data['feature'], return_tensors=\"pt\") \n",
    "# image_tensor = inputs[\"pixel_values\"] \n",
    "image_embeds = image_encoder(data['feature'].unsqueeze(0).to(device)).image_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image  = Image.open(r\"F:\\Projects\\diffusers\\ProgramData\\sample_new\\NPC_Avatar_Girl_Sword_Ayaka\\capture_3.png\").convert('RGB')\n",
    "image = image.resize((256,256))\n",
    "from matplotlib import pyplot as plt\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusionPipeline, StableDiffusionImg2ImgPipeline, DDIMScheduler, AutoencoderKL\n",
    "# pipe = StableDiffusionImg2ImgPipeline.from_pretrained(\n",
    "#     base_model_path,\n",
    "#     torch_dtype=torch.float16, \n",
    "#     scheduler=noise_scheduler,\n",
    "#     vae=vae,\n",
    "#     safety_checker=None\n",
    "# )\n",
    "base_model_path = r'F:\\Projects\\diffusers\\ProgramData\\basemodel'\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    base_model_path,\n",
    "    torch_dtype=torch.float16, \n",
    "    # scheduler=noise_scheduler,\n",
    "    # vae=vae,\n",
    "    safety_checker=None\n",
    ")\n",
    "pipe.to(\"cuda\")  # 把整个模型转移到 GPU\n",
    "\n",
    "\n",
    "# 运行 pipeline\n",
    "image = pipe(prompt=\"1girl,black_hair, blush, closed_eyes, dress, elbow_gloves, breasts, gloves, heart, hug, looking_at_viewer, open_mouth\").images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.imshow(image[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from diffusers.models.resnet import ResnetBlock2D, Upsample2D\n",
    "import sys\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from poseCtrl.data.dataset import load_base_points\n",
    "from PIL import Image\n",
    "import cv2\n",
    "class VPmatrixPointsV1(nn.Module):\n",
    "    \"\"\" \n",
    "    Input:  \n",
    "        V_matrix: [batch,4,4]\n",
    "        P_matrix: [batch,4,4]\n",
    "        raw_base_points: [13860,4]\n",
    "    Output:\n",
    "        base_points: [batch,77,768]\n",
    "    \"\"\"\n",
    "    def __init__(self, raw_base_points,image_width = 512,image_height=512):\n",
    "        super().__init__() \n",
    "        self.register_buffer(\"raw_base_points\", raw_base_points)\n",
    "        self.image_width = image_width\n",
    "        self.image_height = image_height\n",
    "\n",
    "    def forward(self, V_matrix, P_matrix):\n",
    "        VP_matrix = torch.bmm(P_matrix, V_matrix)  # [batch, 4, 4]\n",
    "        points = self.raw_base_points.unsqueeze(0).expand(VP_matrix.shape[0], -1, -1)\n",
    "        transformed_points = torch.bmm(points, VP_matrix.transpose(1, 2))  # [batch, 13860, 4]\n",
    "        transformed_points[..., :3] = torch.where(\n",
    "            transformed_points[..., 3:4] != 0,\n",
    "            transformed_points[..., :3] / transformed_points[..., 3:4],\n",
    "            transformed_points[..., :3]  \n",
    "        ) # [batch, 13860, 3]\n",
    "        transformed_points = transformed_points[..., :3]\n",
    "        image_width, image_height = self.image_width, self.image_height\n",
    "\n",
    "        screen_coords = transformed_points.clone()\n",
    "        screen_coords[..., 0] = (screen_coords[..., 0] + 1) * 0.5 * image_width   # X: [-1,1] -> [0,512]\n",
    "        screen_coords[..., 1] = (1 - (screen_coords[..., 1] + 1) * 0.5) * image_height  # Y 翻转: [-1,1] -> [512,0]\n",
    "\n",
    "        screen_coords = screen_coords.round().long()  # [batch, 13860, 3]\n",
    "\n",
    "        batch_size = screen_coords.shape[0]\n",
    "        tensor_images = torch.zeros((batch_size, 3, image_height, image_width), dtype=torch.uint8)\n",
    "\n",
    "        for b in range(batch_size):\n",
    "            pixels = screen_coords[b].cpu().numpy()\n",
    "            image_array = np.full((image_height, image_width), 255, dtype=np.uint8)\n",
    "\n",
    "            for x, y, _ in pixels:\n",
    "                if 0 <= x < image_width and 0 <= y < image_height:\n",
    "                    image_array[y, x] = 0  \n",
    "            inverted_array = 255 - image_array\n",
    "            kernel = np.ones((3, 3), np.uint8)  \n",
    "            dilated_image = cv2.dilate(inverted_array, kernel, iterations=1)  \n",
    "            smoothed_image = cv2.GaussianBlur(dilated_image, (7, 7), 0)\n",
    "            _, binary_mask = cv2.threshold(smoothed_image, 100, 255, cv2.THRESH_BINARY)\n",
    "            binary_mask_3ch = np.stack([binary_mask] * 3, axis=-1)  # [512, 512, 3]\n",
    "            tensor_images[b] = torch.from_numpy(binary_mask_3ch).permute(2, 0, 1)\n",
    "        return tensor_images.float() / 255\n",
    "    \n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from poseCtrl.data.dataset import CustomDataset\n",
    "\n",
    "path = r\"F:\\\\Projects\\\\diffusers\\\\ProgramData\\\\sample_new\"\n",
    "dataset = CustomDataset(path)\n",
    "data = dataset[0]\n",
    "\n",
    "# # Generate VP Matrix\n",
    "# vp_matrix = data['projection_matrix'] @ data['view_matrix']\n",
    "# model = VPmatrixEncoder()\n",
    "# vp_matrix_tensor = vp_matrix.float().unsqueeze(0)\n",
    "\n",
    "# # Model Testing\n",
    "# model = VPmatrixEncoder()\n",
    "# output = model(vp_matrix_tensor)\n",
    "\n",
    "# print(\"Input shape:\", vp_matrix_tensor.shape)  # Expected: (1, 1, 4, 4)\n",
    "# print(\"Output shape:\", output.shape)  # Expected: (1, 77, 77)\n",
    "\n",
    "\n",
    "path=r'F:\\Projects\\diffusers\\Project\\PoseCtrl\\dataSet\\standardVertex.txt'\n",
    "raw_base_points=load_base_points(path)\n",
    "points = VPmatrixPointsV1(raw_base_points)\n",
    "with torch.no_grad():\n",
    "    base_points=points(data['view_matrix'].unsqueeze(0), data['projection_matrix'].unsqueeze(0))\n",
    "print(base_points.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_points = base_points.float() / 255  # 转换为 float 并归一化\n",
    "\n",
    "torch.max(base_points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_pil = transforms.ToPILImage()(base_points[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchvision import transforms\n",
    "# feature = Image.open(r\"F:\\Projects\\diffusers\\ProgramData\\pic\\105901_unit (1)\\capture_0.png\").convert('RGB')\n",
    "# # feature.shape\n",
    "# transform = transforms.Compose([\n",
    "#             transforms.Resize((512, 512)),  \n",
    "#             transforms.ToTensor(), \n",
    "#         ])\n",
    "# feature = transform(base_points)\n",
    "# feature.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPTextModel, CLIPTokenizer, CLIPVisionModelWithProjection, CLIPProcessor\n",
    "image_encoder = CLIPVisionModelWithProjection.from_pretrained(\"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\")\n",
    "processor = CLIPProcessor.from_pretrained(\"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\")\n",
    "inputs = processor(images=base_points, return_tensors=\"pt\") \n",
    "image_tensor = inputs[\"pixel_values\"]\n",
    "point_embeds = image_encoder(image_tensor).image_embeds\n",
    "# point_embeds.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def forward(V_matrix, P_matrix, raw_base_points, image_width=512, image_height=512):\n",
    "    \"\"\"\n",
    "    处理 3D 点云数据，投影到 2D 屏幕坐标，并对每个 batch 生成平滑的 mask 图像。\n",
    "    返回所有 batch 处理后的 Tensor。\n",
    "    \"\"\"\n",
    "    # 计算视图-投影矩阵\n",
    "    VP_matrix = torch.bmm(P_matrix, V_matrix)  # [batch, 4, 4]\n",
    "\n",
    "    # 扩展点云维度\n",
    "    points = raw_base_points.unsqueeze(0).expand(VP_matrix.shape[0], -1, -1)\n",
    "\n",
    "    # 进行变换\n",
    "    transformed_points = torch.bmm(points, VP_matrix.transpose(1, 2))  # [batch, num_points, 4]\n",
    "\n",
    "    # 透视除法\n",
    "    transformed_points[..., :3] = torch.where(\n",
    "        transformed_points[..., 3:4] != 0,\n",
    "        transformed_points[..., :3] / transformed_points[..., 3:4],\n",
    "        transformed_points[..., :3]  \n",
    "    )\n",
    "\n",
    "    # 只保留 (x, y, z) 部分\n",
    "    transformed_points = transformed_points[..., :3]\n",
    "\n",
    "    # 归一化到屏幕坐标\n",
    "    screen_coords = transformed_points.clone()\n",
    "    screen_coords[..., 0] = (screen_coords[..., 0] + 1) * 0.5 * image_width   # X: [-1,1] -> [0,512]\n",
    "    screen_coords[..., 1] = (1 - (screen_coords[..., 1] + 1) * 0.5) * image_height  # Y 需要翻转\n",
    "\n",
    "    # 取整得到像素坐标\n",
    "    screen_coords = screen_coords.round().long()  # [batch, num_points, 3]\n",
    "\n",
    "    # 存储 batch 处理后的图像\n",
    "    batch_size = screen_coords.shape[0]\n",
    "    tensor_images = torch.zeros((batch_size, image_height, image_width), dtype=torch.uint8)\n",
    "\n",
    "    for b in range(batch_size):\n",
    "        pixels = screen_coords[b].cpu().numpy()\n",
    "\n",
    "        # 创建白色背景图像\n",
    "        image_array = np.full((image_height, image_width), 255, dtype=np.uint8)\n",
    "\n",
    "        # 将点绘制到图像上\n",
    "        for x, y, _ in pixels:\n",
    "            if 0 <= x < image_width and 0 <= y < image_height:\n",
    "                image_array[y, x] = 0  # 黑色点\n",
    "\n",
    "        # 反转黑白颜色\n",
    "        inverted_array = 255 - image_array\n",
    "\n",
    "        # 进行膨胀操作（扩充点，使其更平滑）\n",
    "        kernel = np.ones((3, 3), np.uint8)  # 使用稍大的膨胀核\n",
    "        dilated_image = cv2.dilate(inverted_array, kernel, iterations=1)  # 膨胀 1 次\n",
    "\n",
    "        # 进行高斯模糊，使边缘更加平滑\n",
    "        smoothed_image = cv2.GaussianBlur(dilated_image, (7, 7), 0)\n",
    "\n",
    "        # 二值化，确保是一个清晰的 mask\n",
    "        _, binary_mask = cv2.threshold(smoothed_image, 100, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "        # 转换为 PyTorch Tensor 并存入 batch 结果\n",
    "        tensor_images[b] = torch.from_numpy(binary_mask)\n",
    "\n",
    "    return tensor_images  # [batch, 512, 512]\n",
    "\n",
    "# 示例调用\n",
    "# output_tensor = forward(V_matrix, P_matrix, raw_base_points)\n",
    "# output_tensor.shape -> [batch, 512, 512]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPImageProcessor\n",
    "from transformers import CLIPTextModel, CLIPTokenizer, CLIPVisionModelWithProjection, CLIPProcessor \n",
    "from PIL import Image\n",
    "from diffusers import UNet2DConditionModel\n",
    "import torch\n",
    "image_encoder = CLIPVisionModelWithProjection.from_pretrained(\"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\")\n",
    "processor = CLIPImageProcessor()\n",
    "image = Image.open(r\"F:\\Projects\\diffusers\\ProgramData\\image_mirror_resized\\batch_00000\\6857739.jpg\").convert(\"RGB\")\n",
    "unet = UNet2DConditionModel.from_pretrained(r'F:\\Projects\\diffusers\\ProgramData\\basemodel', subfolder=\"unet\")\n",
    "inputs = processor(images=image, return_tensors=\"pt\").pixel_values\n",
    "print(inputs.shape)  # 输出形状应为 [1, 3, 224, 224]，表示一个图像的输入张量\n",
    "image_embeds = image_encoder(inputs).image_embeds\n",
    "\n",
    "class ImageProjModel(torch.nn.Module):\n",
    "    def __init__(self, cross_attention_dim=1024, clip_embeddings_dim=1024, clip_extra_context_tokens=4):\n",
    "        super().__init__()\n",
    "\n",
    "        self.generator = None\n",
    "        self.cross_attention_dim = cross_attention_dim\n",
    "        self.clip_extra_context_tokens = clip_extra_context_tokens\n",
    "        self.proj = torch.nn.Linear(clip_embeddings_dim, self.clip_extra_context_tokens * cross_attention_dim)\n",
    "        self.norm = torch.nn.LayerNorm(cross_attention_dim)\n",
    "\n",
    "    def forward(self, image_embeds):\n",
    "        embeds = image_embeds\n",
    "        clip_extra_context_tokens = self.proj(embeds).reshape(\n",
    "            -1, self.clip_extra_context_tokens, self.cross_attention_dim\n",
    "        )\n",
    "        clip_extra_context_tokens = self.norm(clip_extra_context_tokens)\n",
    "        return clip_extra_context_tokens\n",
    "    \n",
    "image_proj_model = ImageProjModel(\n",
    "        cross_attention_dim=unet.config.cross_attention_dim,\n",
    "        clip_embeddings_dim=image_encoder.config.projection_dim,\n",
    "        clip_extra_context_tokens=4,\n",
    "    )\n",
    "\n",
    "feature_tokens = image_proj_model(image_embeds)\n",
    "print(feature_tokens.shape)  # 输出形状应为 [1, 4, 1024]，表示 4 个额外的上下文 token，每个 token 的维度为 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "image_embeds\n",
    "image_embeds_t = nn.functional.normalize(image_embeds, dim=-1)\n",
    "loss = torch.nn.functional.mse_loss(image_embeds_t, image_embeds)\n",
    "print(loss.item())  # 输出损失值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoder = CLIPTextModel.from_pretrained(r'F:\\Projects\\diffusers\\ProgramData\\basemodel', subfolder=\"text_encoder\")\n",
    "tokenizer = CLIPTokenizer.from_pretrained(r'F:\\Projects\\diffusers\\ProgramData\\basemodel', subfolder=\"tokenizer\")\n",
    "text='a highly detailed anime girl, in front of a pure black background'\n",
    "text_input_ids = tokenizer(\n",
    "            text,\n",
    "            max_length=tokenizer.model_max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        ).input_ids\n",
    "\n",
    "encoder_hidden_states = text_encoder(text_input_ids)[0]\n",
    "encoder_hidden_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(r'F:\\Projects\\diffusers\\Project\\PoseCtrl')\n",
    "sys.path.append(r'F:\\Projects\\diffusers\\Project\\PoseCtrl\\poseCtrl')\n",
    "from poseCtrl.data.dataset import CustomDataset_v4, load_base_points, CombinedDataset\n",
    "from transformers import CLIPImageProcessor\n",
    "from transformers import CLIPTextModel, CLIPTokenizer, CLIPVisionModelWithProjection, CLIPProcessor \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "tokenizer = CLIPTokenizer.from_pretrained(r'F:\\Projects\\diffusers\\ProgramData\\basemodel', subfolder=\"tokenizer\")\n",
    "data_root_path_2 = r'F:\\Projects\\diffusers\\Backup\\image_mirror_resized'\n",
    "train_dataset = CombinedDataset(\n",
    "    # path1=args.data_root_path_1,\n",
    "    path2=data_root_path_2,\n",
    "    # path3=args.data_root_path_3,\n",
    "    # path4=args.data_root_path_4,\n",
    "    # path5=args.data_root_path_5,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Flatten(),           # (B, 4, 4) -> (B, 16)\n",
    "            nn.Linear(16, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1024)    # (B, 1024)\n",
    "        )\n",
    "    def forward(self, vp_matrix):\n",
    "        return self.model(vp_matrix)\n",
    "    \n",
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 16),     # (B, 16)\n",
    "            nn.Unflatten(1, (4, 4)) # (B, 4, 4)\n",
    "        )\n",
    "    def forward(self, image_embeds):\n",
    "        \"\"\"\n",
    "        image_embeds: [batch, 1024]\n",
    "        \"\"\"\n",
    "        return self.model(image_embeds)\n",
    "    \n",
    "class Autoencodermodel(torch.nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.Encoder = Encoder() #Encoder\n",
    "        self.Decoder = Decoder() #Decoder\n",
    "        self.clipencoder = CLIPVisionModelWithProjection.from_pretrained(\"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\").to(\"cuda\")\n",
    "        # self.set_training()\n",
    "    \n",
    "    def set_training(self):\n",
    "        self.Encoder.require_grad_(True)\n",
    "        self.Decoder.require_grad_(True)\n",
    "        self.clipencoder.require_grad_(False)\n",
    "    \n",
    "    def forward(self, images, v_matrix, p_matrix):\n",
    "        \"\"\"\n",
    "        images: [batch, 3, 512, 512]\n",
    "        v_matrix: [batch, 4, 4]\n",
    "        p_matrix: [batch, 4, 4]\n",
    "        \"\"\"\n",
    "        images = (images + 1.0) / 2.0\n",
    "        inputs = processor(images=images, return_tensors=\"pt\").pixel_values.to(\"cuda\")\n",
    "        image_embeds = self.clipencoder(inputs).image_embeds\n",
    "\n",
    "\n",
    "        vp_matrix = torch.bmm(p_matrix, v_matrix) \n",
    "        encoded_vp = self.Encoder(vp_matrix)\n",
    "\n",
    "        compare_vp = self.Decoder(image_embeds)\n",
    "\n",
    "        \n",
    "        return compare_vp, encoded_vp, image_embeds\n",
    "\n",
    "\n",
    "model = Autoencodermodel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import ToPILImage\n",
    "images = train_dataset[0]['image']\n",
    "images = (images + 1.0) / 2.0\n",
    "image = ToPILImage()(images)\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "# 假设 90% train，10% test\n",
    "train_len = int(len(train_dataset) * 0.9)\n",
    "test_len = len(train_dataset) - train_len\n",
    "train_set, test_set = random_split(train_dataset, [train_len, test_len])\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=32, shuffle=True, num_workers=4)\n",
    "test_loader = DataLoader(test_set, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "test_iter = iter(test_loader)  # 用于每个batch后测试\n",
    "\n",
    "\n",
    "# 初始化模型\n",
    "model = Autoencodermodel()\n",
    "model.to(\"cuda\")\n",
    "checkpoint = torch.load(r\"F:\\Projects\\diffusers\\Backup\\ckpt\\autoencoder_checkpoint.pth\", map_location=\"cuda\")\n",
    "model.Encoder.load_state_dict(checkpoint['encoder'])\n",
    "model.Decoder.load_state_dict(checkpoint['decoder'])\n",
    "# 冻结 clipencoder，仅训练 Encoder 和 Decoder\n",
    "model.clipencoder.eval()\n",
    "model.clipencoder.requires_grad_(False)\n",
    "model.Encoder.requires_grad_(True)\n",
    "model.Decoder.requires_grad_(True)\n",
    "model.Encoder.train()\n",
    "model.Decoder.train()\n",
    "save_root = r\"F:\\Projects\\diffusers\\Backup\\ckpt\"\n",
    "# 损失函数\n",
    "mse_loss = nn.MSELoss()\n",
    "\n",
    "# 优化器（只训练 Encoder 和 Decoder 参数）\n",
    "optimizer = optim.Adam(\n",
    "    list(model.Encoder.parameters()) + list(model.Decoder.parameters()), \n",
    "    lr=1e-4\n",
    ")\n",
    "\n",
    "# processor 初始化（匹配你用的 clip 模型）\n",
    "processor = CLIPProcessor.from_pretrained(\"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\")\n",
    "\n",
    "# 训练循环\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "\n",
    "    for batch in pbar:\n",
    "        model.train()\n",
    "        images = batch[\"image\"].to(\"cuda\")\n",
    "        v_matrix = batch[\"view_matrix\"].to(\"cuda\")\n",
    "        p_matrix = batch[\"projection_matrix\"].to(\"cuda\")\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        compare_vp, encoded_vp, image_embeds = model(images, v_matrix, p_matrix)\n",
    "        image_embeds = nn.functional.normalize(image_embeds, dim=-1)\n",
    "        encoded_vp = nn.functional.normalize(encoded_vp, dim=-1)\n",
    "        logits = torch.matmul(encoded_vp, image_embeds.T) / 0.07\n",
    "        labels = torch.arange(logits.size(0)).to(logits.device)\n",
    "        loss_align = nn.CrossEntropyLoss()(logits, labels) + nn.CrossEntropyLoss()(logits.T, labels)\n",
    "        loss_reconstruct = mse_loss(compare_vp, torch.bmm(p_matrix, v_matrix))\n",
    "        # loss_align = mse_loss(encoded_vp, image_embeds)\n",
    "        loss = 20 * loss_reconstruct + loss_align\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # ==== 测试 ====\n",
    "        model.eval()\n",
    "        try:\n",
    "            test_batch = next(test_iter)\n",
    "        except StopIteration:\n",
    "            test_iter = iter(test_loader)\n",
    "            test_batch = next(test_iter)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            images_t = test_batch[\"image\"].to(\"cuda\")\n",
    "            v_matrix_t = test_batch[\"view_matrix\"].to(\"cuda\")\n",
    "            p_matrix_t = test_batch[\"projection_matrix\"].to(\"cuda\")\n",
    "\n",
    "            compare_vp_t, encoded_vp_t, image_embeds_t = model(images_t, v_matrix_t, p_matrix_t)\n",
    "            # 标准化特征向量（batch, 1024）\n",
    "            image_embeds_t = nn.functional.normalize(image_embeds_t, dim=-1)\n",
    "            encoded_vp_t = nn.functional.normalize(encoded_vp_t, dim=-1)\n",
    "            # 余弦相似度矩阵： (B, B)，对角线是正样本对\n",
    "            logits_t = torch.matmul(encoded_vp_t, image_embeds_t.T)  # (B, B)\n",
    "            temperature_t = 0.07\n",
    "            logits_t /= temperature_t\n",
    "            labels_t = torch.arange(logits_t.size(0)).to(logits_t.device)\n",
    "\n",
    "            loss_align_t = nn.CrossEntropyLoss()(logits_t, labels_t) + nn.CrossEntropyLoss()(logits_t.T, labels_t)\n",
    "\n",
    "            loss_recon_t = mse_loss(compare_vp_t, torch.bmm(p_matrix_t, v_matrix_t))\n",
    "            # loss_align_t = mse_loss(encoded_vp_t, image_embeds_t)\n",
    "            loss_test = 20 * loss_recon_t +loss_align_t\n",
    "\n",
    "        # ==== 打印 ====\n",
    "        pbar.set_postfix({\n",
    "            \"train_loss\": loss.item(),\n",
    "            \"test_loss\": loss_test.item(),\n",
    "            \"recon_t\": loss_recon_t.item(),\n",
    "            \"align_t\": loss_align_t.item()\n",
    "        })\n",
    "    epoch_dir = os.path.join(save_root, f\"epoch_{epoch+1}\")\n",
    "    os.makedirs(epoch_dir, exist_ok=True)\n",
    "\n",
    "    save_path = os.path.join(epoch_dir, \"autoencoder_checkpoint.pth\")\n",
    "    torch.save({\n",
    "        'encoder': model.Encoder.state_dict(),\n",
    "        'decoder': model.Decoder.state_dict(),\n",
    "        'epoch': epoch + 1\n",
    "    }, save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 重新初始化模型并加载权重 ===\n",
    "model = Autoencodermodel()\n",
    "model.Decoder = Decoder()  # 不要忘记补 decoder\n",
    "model.to(\"cuda\")\n",
    "\n",
    "checkpoint = torch.load(r\"F:\\Projects\\diffusers\\Backup\\ckpt\\autoencoder_checkpoint.pth\", map_location=\"cuda\")\n",
    "model.Encoder.load_state_dict(checkpoint['encoder'])\n",
    "model.Decoder.load_state_dict(checkpoint['decoder'])\n",
    "model.eval()\n",
    "model.clipencoder.eval()\n",
    "print(f\"Loaded checkpoint from epoch {checkpoint['epoch']}\")\n",
    "# 从 test_set 中取一个样本\n",
    "sample = train_dataset[2]\n",
    "\n",
    "# 转为 batch 格式 + 放到 cuda\n",
    "image = sample[\"image\"].unsqueeze(0).to(\"cuda\")              # [1, 3, H, W]\n",
    "v_matrix = sample[\"view_matrix\"].unsqueeze(0).to(\"cuda\")     # [1, 4, 4]\n",
    "p_matrix = sample[\"projection_matrix\"].unsqueeze(0).to(\"cuda\")  # [1, 4, 4]\n",
    "\n",
    "# 获取 processor（必须使用 CLIPProcessor）\n",
    "from transformers import CLIPProcessor\n",
    "processor = CLIPProcessor.from_pretrained(\"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\")\n",
    "\n",
    "# 转 PIL 图像（必要）\n",
    "from torchvision import transforms\n",
    "image_pil = transforms.ToPILImage()(image[0].cpu())\n",
    "inputs = processor(images=image_pil, return_tensors=\"pt\").pixel_values.to(\"cuda\")\n",
    "\n",
    "# 前向传播\n",
    "with torch.no_grad():\n",
    "    image_embeds = model.clipencoder(inputs).image_embeds\n",
    "    vp_matrix = torch.bmm(p_matrix, v_matrix)\n",
    "    encoded_vp = model.Encoder(vp_matrix)\n",
    "    decoded_vp = model.Decoder(image_embeds)\n",
    "    \n",
    "    loss_recon = nn.MSELoss()(decoded_vp, vp_matrix)\n",
    "    loss_align = nn.MSELoss()(encoded_vp, image_embeds)\n",
    "    print(f\"[Single Test] Recon Loss: {loss_recon.item():.4f}, Align Loss: {loss_align.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_vp = nn.functional.normalize(encoded_vp, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_embeds = nn.functional.normalize(image_embeds, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_embeds_ = image_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "similarity = F.cosine_similarity(encoded_vp, image_embeds_, dim=-1).mean() \n",
    "similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vp_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_vp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def merge_joints2d(root_dir, output_path):\n",
    "    with open(output_path, 'w', encoding='utf-8') as out_file:\n",
    "        for folder_name in os.listdir(root_dir):\n",
    "            folder_path = os.path.join(root_dir, folder_name)\n",
    "            if not os.path.isdir(folder_path):\n",
    "                continue\n",
    "\n",
    "            joints_path = os.path.join(folder_path, 'selected_joints2d.txt')\n",
    "            if os.path.exists(joints_path):\n",
    "                out_file.write(f\"# {folder_name}\\n\")  # 写入图像文件名\n",
    "                with open(joints_path, 'r', encoding='utf-8') as f:\n",
    "                    for line in f:\n",
    "                        out_file.write(line.strip() + '\\n')\n",
    "                out_file.write('\\n')  # 每组之间加空行\n",
    "\n",
    "# 使用示例\n",
    "root_folder = r\"F:\\Projects\\diffusers\\ProgramData\\points\\output_2\\output\"\n",
    "output_txt = r\"F:\\Projects\\diffusers\\ProgramData\\points\\output_2\\output/merged_joints2d.txt\"\n",
    "merge_joints2d(root_folder, output_txt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting: 100%|██████████| 155970/155970 [01:57<00:00, 1326.50item/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "def collect_all_paths(folder):\n",
    "    paths = []\n",
    "    for root, dirs, files in os.walk(folder, topdown=False):\n",
    "        for f in files:\n",
    "            paths.append(os.path.join(root, f))\n",
    "        for d in dirs:\n",
    "            paths.append(os.path.join(root, d))\n",
    "    paths.append(folder)  # 最后删除顶层文件夹自身\n",
    "    return paths\n",
    "\n",
    "def delete_folder_with_progress(folder_path):\n",
    "    if not os.path.exists(folder_path):\n",
    "        print(\"路径不存在\")\n",
    "        return\n",
    "\n",
    "    all_items = collect_all_paths(folder_path)\n",
    "\n",
    "    for path in tqdm(all_items, desc=\"Deleting\", unit=\"item\"):\n",
    "        try:\n",
    "            if os.path.isfile(path) or os.path.islink(path):\n",
    "                os.remove(path)\n",
    "            elif os.path.isdir(path):\n",
    "                os.rmdir(path)\n",
    "        except Exception as e:\n",
    "            print(f\"无法删除: {path}, 错误: {e}\")\n",
    "\n",
    "# 使用示例\n",
    "folder_to_delete = r\"F:\\Projects\\diffusers\\ProgramData\\02_output\"\n",
    "delete_folder_with_progress(folder_to_delete)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from path2: F:\\Projects\\diffusers\\ProgramData\\test\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(r'F:\\Projects\\diffusers\\Project\\PoseCtrl')\n",
    "sys.path.append(r'F:\\Projects\\diffusers\\Project\\PoseCtrl\\poseCtrl')\n",
    "from poseCtrl.data.dataset import CustomDataset_v4, load_base_points, CombinedDataset, CombinedDatasetTest\n",
    "from transformers import CLIPImageProcessor\n",
    "from transformers import CLIPTextModel, CLIPTokenizer, CLIPVisionModelWithProjection, CLIPProcessor \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "processor = CLIPImageProcessor()\n",
    "tokenizer = CLIPTokenizer.from_pretrained(r'F:\\Projects\\diffusers\\ProgramData\\basemodel', subfolder=\"tokenizer\")\n",
    "data_root_path_2 = r\"F:\\Projects\\diffusers\\ProgramData\\test\"\n",
    "txt_subdir_name = r'F:\\Projects\\diffusers\\ProgramData\\new_data\\image\\smpl'\n",
    "train_dataset = CombinedDatasetTest(\n",
    "    # path1=args.data_root_path_1,\n",
    "    path2=data_root_path_2,\n",
    "    # path3=data_root_path_3,\n",
    "    # path4=args.data_root_path_4,\n",
    "    # path5=args.data_root_path_5,\n",
    "    tokenizer=tokenizer,\n",
    "    txt_subdir_name=txt_subdir_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10475, 3])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[4]['points'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 77, 768])\n",
      "torch.Size([1, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "class STNkd(nn.Module):\n",
    "    def __init__(self, k=64):\n",
    "        super(STNkd, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv1d(k, 64, 1)\n",
    "        self.conv2 = torch.nn.Conv1d(64, 128, 1)\n",
    "        self.conv3 = torch.nn.Conv1d(128, 1024, 1)\n",
    "        self.fc1 = nn.Linear(1024, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, k * k)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.bn3 = nn.BatchNorm1d(1024)\n",
    "        self.bn4 = nn.BatchNorm1d(512)\n",
    "        self.bn5 = nn.BatchNorm1d(256)\n",
    "\n",
    "        self.k = k\n",
    "\n",
    "    def forward(self, x):\n",
    "        batchsize = x.size()[0]\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = torch.max(x, 2, keepdim=True)[0]\n",
    "        x = x.view(-1, 1024)\n",
    "        x = F.relu(self.bn4(self.fc1(x)))\n",
    "        x = F.relu(self.bn5(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        iden = Variable(torch.from_numpy(np.eye(self.k).flatten().astype(np.float32))).view(1, self.k * self.k).repeat(\n",
    "            batchsize, 1)\n",
    "        if x.is_cuda:\n",
    "            iden = iden.cuda()\n",
    "        x = x + iden\n",
    "        x = x.view(-1, self.k, self.k)\n",
    "        return x\n",
    "    \n",
    "class PointNetEncoder(nn.Module):\n",
    "    def __init__(self, channel=3):\n",
    "        super(PointNetEncoder, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv1d(channel, 64, 1)\n",
    "        self.conv2 = torch.nn.Conv1d(64, 128, 1)\n",
    "        self.conv3 = torch.nn.Conv1d(128, 1024, 1)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.bn3 = nn.BatchNorm1d(1024)\n",
    "        self.fstn = STNkd(k=64)\n",
    "\n",
    "        self.feature_proj = nn.Linear(10475, 768)\n",
    "        self.seq_proj = nn.Linear(1024, 77)\n",
    "        self.norm = nn.LayerNorm(768)\n",
    "        self.act = nn.GELU()\n",
    "\n",
    "    def forward(self, x, V_matrix, P_matrix):\n",
    "        B, D, N = x.size()\n",
    "        trans = torch.bmm(P_matrix, V_matrix) \n",
    "        new_dim = torch.ones(B, D, 1, device=x.device)\n",
    "        x = torch.cat([x, new_dim], dim=2)\n",
    "        x = torch.bmm(x, trans.transpose(1, 2))\n",
    "        x[..., :3] = torch.where(\n",
    "            x[..., 3:4] != 0,\n",
    "            x[..., :3] / x[..., 3:4],\n",
    "            x[..., :3]\n",
    "        )  # [batch, 13860, 3]\n",
    "        x = x[..., :3]\n",
    "        x = x.transpose(2, 1)\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        trans_feat = self.fstn(x)\n",
    "        x = x.transpose(2, 1)\n",
    "        x = torch.bmm(x, trans_feat)\n",
    "        x = x.transpose(2, 1)\n",
    "        pointfeat = x\n",
    "\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.bn3(self.conv3(x))\n",
    "        x = self.feature_proj(x)      # [b, 1024, 768]\n",
    "        x = self.norm(x)\n",
    "        x = self.act(x)\n",
    "        x = x.transpose(1, 2)         # [b, 768, 1024]\n",
    "        x = self.seq_proj(x)          # [b, 768, 77]\n",
    "        x = x.transpose(1, 2)\n",
    "        return x, trans_feat\n",
    "        \n",
    "pointnet_encoder = PointNetEncoder(channel=3).to(\"cuda\")\n",
    "pointnet_encoder.eval()\n",
    "input_points = train_dataset[4]['points'].unsqueeze(0).to(\"cuda\")  # [1, 3, N]\n",
    "v_matrix = train_dataset[4]['view_matrix'].unsqueeze(0).to(\"cuda\")     # [1, 4, 4]\n",
    "p_matrix = train_dataset[4]['projection_matrix'].unsqueeze(0).to(\"cuda\")  # [1, 4, 4]\n",
    "with torch.no_grad():\n",
    "    point  = pointnet_encoder(input_points, v_matrix, p_matrix)\n",
    "print(point[0].shape)  # [1, 1024]\n",
    "print(point[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.5578, device='cuda:0')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def feature_transform_reguliarzer(trans):\n",
    "    d = trans.size()[1]\n",
    "    I = torch.eye(d)[None, :, :]\n",
    "    if trans.is_cuda:\n",
    "        I = I.cuda()\n",
    "    loss = torch.mean(torch.norm(torch.bmm(trans, trans.transpose(2, 1)) - I, dim=(1, 2)))\n",
    "    return loss\n",
    "ft_loss = feature_transform_reguliarzer(point[1])\n",
    "ft_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "- atten_pro\n",
    "- inference\n",
    "- dataset check\n",
    "- training debug"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cameractrl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
