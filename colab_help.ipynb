{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 把底模safetensors转化为文件夹权重模式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionPipeline\n",
    "\n",
    "checkpoint_path = r\"F:\\Projects\\diffusers\\ProgramData\\tmndMix_tmndMixSPRAINBOW.safetensors\"\n",
    "save_path = r\"F:\\Projects\\diffusers\\ProgramData\\basemodel\"\n",
    "# 加载 .safetensors 文件\n",
    "pipeline = StableDiffusionPipeline.from_single_file(checkpoint_path)\n",
    "\n",
    "# 将模型保存为 diffusers 格式\n",
    "pipeline.save_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 把数据集解压"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get install unrar\n",
    "!unrar x /content/drive/MyDrive/pic.rar -o+ /content/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 训练\n",
    "\n",
    "在train_colab.py里面改路径\n",
    "\n",
    "sys.path.append(r'F:\\Projects\\diffusers\\Project\\PoseCtrl')\n",
    "\n",
    "sys.path.append(r'F:\\Projects\\diffusers\\Project\\PoseCtrl\\poseCtrl')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "训练：\n",
    "python train_colab.py --pretrained_model_name_or_path 底模路径 --base_point_path 原始点路径 --data_root_path 数据集路径 --learning_rate 学习率 --save_steps 权重保留步数 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 把模型大权重转化为训练部分小权重model.safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch\n",
    "from pathlib import Path\n",
    "def change_checkpoint(checkpoint_path, new_checkpoint_path):\n",
    "    sd = torch.load(checkpoint_path, map_location=\"cpu\")\n",
    "    vpmatrix_points_sd = {}\n",
    "    atten_sd = {}\n",
    "    proj_sd={}\n",
    "    for k in sd:\n",
    "        if k.startswith(\"unet\"):\n",
    "            pass\n",
    "        elif k.startswith(\"vpmatrix_points\"):\n",
    "            vpmatrix_points_sd[k.replace(\"vpmatrix_points.\", \"\")] = sd[k]\n",
    "        elif k.startswith(\"atten_modules\"):\n",
    "            atten_sd[k.replace(\"atten_modules.\", \"\")] = sd[k]\n",
    "        elif k.startswith(\"image_proj_model\"):\n",
    "            proj_sd[k.replace(\"image_proj_model.\", \"\")] = sd[k]\n",
    "    new_checkpoint_path = Path(new_checkpoint_path, \"posectrl.bin\")\n",
    "    torch.save({\"vpmatrix_points\": vpmatrix_points_sd, \"atten_modules\": atten_sd, \"image_proj_model\": proj_sd}, new_checkpoint_path)\n",
    "    print(f\"Saved new checkpoint to {new_checkpoint_path}\")\n",
    "\n",
    "ckpt = r\"F:\\Projects\\diffusers\\Project\\PoseCtrl\\sd-pose_ctrl\\model.safetensors\"\n",
    "\n",
    "change_checkpoint(ckpt, r\"/content/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. oom的时候清内存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ps aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kill -9 13646 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. inference I2I 先改sys.append再改路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch\n",
    "from diffusers import StableDiffusionPipeline, StableDiffusionImg2ImgPipeline, StableDiffusionInpaintPipelineLegacy, DDIMScheduler, AutoencoderKL\n",
    "from PIL import Image\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(r'F:\\Projects\\diffusers\\Project\\PoseCtrl')\n",
    "sys.path.append(r'F:\\Projects\\diffusers\\Project\\PoseCtrl\\poseCtrl')\n",
    "from poseCtrl.models.pose_adaptor import VPmatrixPoints, ImageProjModel\n",
    "from poseCtrl.models.attention_processor import AttnProcessor, PoseAttnProcessor\n",
    "from poseCtrl.data.dataset import CustomDataset, load_base_points\n",
    "from poseCtrl.models.posectrl import PoseCtrl\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "base_point_path=r'F:\\Projects\\diffusers\\Project\\PoseCtrl\\dataSet\\standardVertex.txt'\n",
    "raw_base_points=load_base_points(base_point_path)  \n",
    "base_model_path = r\"F:\\Projects\\diffusers\\ProgramData\\basemodel\"\n",
    "vae_model_path = \"stabilityai/sd-vae-ft-mse\"\n",
    "image_encoder_path = \"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\"\n",
    "ip_ckpt = r\"F:\\Projects\\diffusers\\Project\\sd-pose_ctrl\\trail_1\\posectrl.bin\"\n",
    "path = r\"F:\\\\Projects\\\\diffusers\\\\ProgramData\\\\sample_new\"\n",
    "device = \"cuda\"\n",
    "\n",
    "def image_grid(imgs, rows, cols):\n",
    "    assert len(imgs) == rows*cols\n",
    "\n",
    "    w, h = imgs[0].size\n",
    "    grid = Image.new('RGB', size=(cols*w, rows*h))\n",
    "    grid_w, grid_h = grid.size\n",
    "    \n",
    "    for i, img in enumerate(imgs):\n",
    "        grid.paste(img, box=(i%cols*w, i//cols*h))\n",
    "    return grid\n",
    "\n",
    "noise_scheduler = DDIMScheduler(\n",
    "    num_train_timesteps=1000,\n",
    "    beta_start=0.00085,\n",
    "    beta_end=0.012,\n",
    "    beta_schedule=\"scaled_linear\",\n",
    "    clip_sample=False,\n",
    "    set_alpha_to_one=False,\n",
    "    steps_offset=1,\n",
    ")\n",
    "vae = AutoencoderKL.from_pretrained(vae_model_path).to(dtype=torch.float16)\n",
    "\n",
    "# load SD pipeline\n",
    "pipe = StableDiffusionImg2ImgPipeline.from_pretrained(\n",
    "    base_model_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    scheduler=noise_scheduler,\n",
    "    vae=vae,\n",
    "    feature_extractor=None,\n",
    "    safety_checker=None\n",
    ")\n",
    "\n",
    "\n",
    "dataset = CustomDataset(path)\n",
    "data = dataset[467]\n",
    "from torchvision import transforms\n",
    "\n",
    "transform = transforms.Resize((256, 256))\n",
    "\n",
    "\n",
    "image = data['image']\n",
    "image_pil = transforms.ToPILImage()(image)\n",
    "image_pil = transform(image_pil) \n",
    "\n",
    "g_image = data['feature']\n",
    "g_image_pil = transforms.ToPILImage()(g_image)\n",
    "g_image_pil = transform(g_image_pil) \n",
    "\n",
    "vmatrix = data['view_matrix'].to(torch.float16).unsqueeze(0).to(device)\n",
    "pmatrix = data['projection_matrix'].to(torch.float16).unsqueeze(0).to(device)\n",
    "\n",
    "pose_model = PoseCtrl(pipe, image_encoder_path, ip_ckpt, raw_base_points, device)\n",
    "images = pose_model.generate(pil_image=g_image_pil, num_samples=4, num_inference_steps=50, seed=37, image=image_pil, strength=0.6, V_matrix=vmatrix,P_matrix=pmatrix )\n",
    "# images = pose_model.generate(pil_image=g_image, num_samples=4, num_inference_steps=50, seed=42, strength=0.6, V_matrix=vmatrix,P_matrix=pmatrix )\n",
    "grid = image_grid(images, 1, 4)\n",
    "grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. inference T2I 先改sys.append再改路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch\n",
    "from diffusers import StableDiffusionPipeline, StableDiffusionImg2ImgPipeline, StableDiffusionInpaintPipelineLegacy, DDIMScheduler, AutoencoderKL\n",
    "from PIL import Image\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(r'F:\\Projects\\diffusers\\Project\\PoseCtrl')\n",
    "sys.path.append(r'F:\\Projects\\diffusers\\Project\\PoseCtrl\\poseCtrl')\n",
    "from poseCtrl.models.pose_adaptor import VPmatrixPoints, ImageProjModel\n",
    "from poseCtrl.models.attention_processor import AttnProcessor, PoseAttnProcessor\n",
    "from poseCtrl.data.dataset import CustomDataset, load_base_points\n",
    "from poseCtrl.models.posectrl import PoseCtrl\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "base_point_path=r'F:\\Projects\\diffusers\\Project\\PoseCtrl\\dataSet\\standardVertex.txt'\n",
    "raw_base_points=load_base_points(base_point_path)  \n",
    "\n",
    "base_model_path = r\"F:\\Projects\\diffusers\\ProgramData\\basemodel\"\n",
    "vae_model_path = \"stabilityai/sd-vae-ft-mse\"\n",
    "image_encoder_path = \"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\"\n",
    "ip_ckpt = r\"F:\\Projects\\diffusers\\Project\\sd-pose_ctrl\\trail_4\\posectrl.bin\"\n",
    "device = \"cuda\"\n",
    "\n",
    "def image_grid(imgs, rows, cols):\n",
    "    assert len(imgs) == rows*cols\n",
    "\n",
    "    w, h = imgs[0].size\n",
    "    grid = Image.new('RGB', size=(cols*w, rows*h))\n",
    "    grid_w, grid_h = grid.size\n",
    "    \n",
    "    for i, img in enumerate(imgs):\n",
    "        grid.paste(img, box=(i%cols*w, i//cols*h))\n",
    "    return grid\n",
    "\n",
    "noise_scheduler = DDIMScheduler(\n",
    "    num_train_timesteps=1000,\n",
    "    beta_start=0.00085,\n",
    "    beta_end=0.012,\n",
    "    beta_schedule=\"scaled_linear\",\n",
    "    clip_sample=False,\n",
    "    set_alpha_to_one=False,\n",
    "    steps_offset=1,\n",
    ")\n",
    "vae = AutoencoderKL.from_pretrained(vae_model_path).to(dtype=torch.float16)\n",
    "\n",
    "# load SD pipeline\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    base_model_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    scheduler=noise_scheduler,\n",
    "    vae=vae,\n",
    "    feature_extractor=None,\n",
    "    safety_checker=None\n",
    ")\n",
    "path = r\"F:\\\\Projects\\\\diffusers\\\\ProgramData\\\\sample_new\"\n",
    "dataset = CustomDataset(path)\n",
    "data = dataset[344]\n",
    "from torchvision import transforms\n",
    "\n",
    "transform = transforms.Resize((256, 256))\n",
    "\n",
    "\n",
    "image = data['image']\n",
    "image_pil = transforms.ToPILImage()(image)\n",
    "image_pil = transform(image_pil)  \n",
    "\n",
    "g_image = data['feature']\n",
    "g_image_pil = transforms.ToPILImage()(g_image)\n",
    "g_image_pil = transform(g_image_pil) \n",
    "\n",
    "vmatrix = data['view_matrix'].to(torch.float16).unsqueeze(0).to(device)\n",
    "pmatrix = data['projection_matrix'].to(torch.float16).unsqueeze(0).to(device)\n",
    "\n",
    "pose_model = PoseCtrl(pipe, image_encoder_path, ip_ckpt, raw_base_points, device)\n",
    "# images = pose_model.generate(pil_image=g_image, num_samples=4, num_inference_steps=50, seed=42, image=image, strength=0.6, V_matrix=vmatrix,P_matrix=pmatrix )\n",
    "images = pose_model.generate(pil_image=g_image, num_samples=4, num_inference_steps=100, seed=41, strength=0.6, V_matrix=vmatrix,P_matrix=pmatrix )\n",
    "grid = image_grid(images, 1, 4)\n",
    "grid\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cameractrl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
